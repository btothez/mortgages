{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"shuffled-full-set-hashed.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = pickle.load(open('../server/webservice/pickles/encoder.pkl', 'rb'))\n",
    "vectorizer = pickle.load(open('../server/webservice/pickles/vectorizer.pkl', 'rb'))\n",
    "lsa = pickle.load(open('../server/webservice/pickles/lsa.pkl', 'rb'))\n",
    "knn_lsa = pickle.load(open('../server/webservice/pickles/knn_lsa.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.5, max_features=10000,\n",
       "                min_df=2, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DELETION OF INTEREST</td>\n",
       "      <td>e04a09c87692 d6b72e591b91 5d066f0246f1 ed41171...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RETURNED CHECK</td>\n",
       "      <td>a3b334c6eefd be95012ebf2b 41d67080e078 ff1c26e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BILL</td>\n",
       "      <td>586242498a88 9ccf259ca087 54709b24b45f 6bf9c0c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BILL</td>\n",
       "      <td>cd50e861f48b 6ca2dd348663 d38820625542 f077614...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BILL</td>\n",
       "      <td>9db5536263d8 1c303d15eb65 3f89b4673455 b73e657...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0                                                  1\n",
       "0  DELETION OF INTEREST  e04a09c87692 d6b72e591b91 5d066f0246f1 ed41171...\n",
       "1        RETURNED CHECK  a3b334c6eefd be95012ebf2b 41d67080e078 ff1c26e...\n",
       "2                  BILL  586242498a88 9ccf259ca087 54709b24b45f 6bf9c0c...\n",
       "3                  BILL  cd50e861f48b 6ca2dd348663 d38820625542 f077614...\n",
       "4                  BILL  9db5536263d8 1c303d15eb65 3f89b4673455 b73e657..."
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = len(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = df[0:10000]\n",
    "sample.dropna(inplace=True)\n",
    "sample_y = sample[0]\n",
    "\n",
    "def get_label_vect(label):\n",
    "    vect = [0] * output_size\n",
    "    vect[label - 1] = 1\n",
    "    return vect\n",
    "encoded_y = np.array([get_label_vect(l) for l in encoder.transform(sample_y)])\n",
    "encoded_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 20\n",
    "def chunks(l, n):\n",
    "    # For item i in a range that is a length of l,\n",
    "    for i in range(0, len(l), n):\n",
    "        # Create an index range for l of n items:\n",
    "        new_chunk = l[i:i+n]\n",
    "        if len(new_chunk) < n:\n",
    "            new_chunk = ([''] * (n - len(new_chunk))) + new_chunk\n",
    "        yield new_chunk\n",
    "        \n",
    "doc_arr = [x for x in sample[1]]\n",
    "doc_arr[0].split(' ')\n",
    "doc_words = [doc.split(' ') for doc in doc_arr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = []\n",
    "len(doc_words), len(encoded_y)\n",
    "for i in range(len(doc_words)):\n",
    "    for chunk in chunks(doc_words[i], chunk_size):\n",
    "        training_set.append((encoded_y[i], chunk))\n",
    "        \n",
    "training_vects = []        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 170380\n"
     ]
    }
   ],
   "source": [
    "# Now for each word, get the tfidf vector\n",
    "def word_2_vect(word):\n",
    "    return lsa.transform(vectorizer.transform(pd.Series([word])))[0]\n",
    "\n",
    "i = 0\n",
    "for label, chunk in training_set:\n",
    "    training_vects.append((label, [word_2_vect(word) for word in chunk]))\n",
    "    if i%500 == 0:\n",
    "        print('{} of {}'.format(i, len(training_set)))\n",
    "    i += 1\n",
    "\n",
    "training_vects[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5337"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_vects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTake about 100 lines, just as a toy example\\nEncode the label\\nBreak the string up into an array of 20 words get the remainder as another chunk with empty str as the first \\n    20 - n\\nFor each word in the array, get its truncated tfidf vector so one of those 20 word arrays should correspond to \\n    a 20x100 array\\nThen build the LSTM\\n'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Take about 100 lines, just as a toy example\n",
    "Encode the label\n",
    "Break the string up into an array of 20 words get the remainder as another chunk with empty str as the first \n",
    "    20 - n\n",
    "For each word in the array, get its truncated tfidf vector so one of those 20 word arrays should correspond to \n",
    "    a 20x100 array\n",
    "Then build the LSTM\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "lstm_layers = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "# Add nodes to the graph\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(\"float\", [None, chunk_size, 100])\n",
    "    labels_ = tf.placeholder(tf.int32, [None, output_size])\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    with tf.name_scope(\"RNN_layers\"):\n",
    "        def lstm_cell():\n",
    "            # Your basic LSTM cell\n",
    "            lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size, reuse=tf.get_variable_scope().reuse)\n",
    "            # Add dropout to the cell\n",
    "            return tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "\n",
    "        # Stack up multiple LSTM layers, for deep learning\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(lstm_layers)])\n",
    "\n",
    "        # Getting an initial state of all zeros\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs_,\n",
    "                                             initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], output_size, activation_fn=tf.sigmoid)\n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5337, 14)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([tup[0] for tup in training_vects])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5337, 20, 100)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([tup[1] for tup in training_vects])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=300):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prop = 0.8\n",
    "\n",
    "set_size = x.shape[0]\n",
    "\n",
    "split_idx = int(set_size*0.8)\n",
    "train_x, rest_x = x[:split_idx], x[split_idx:]\n",
    "train_y, rest_y = y[:split_idx], y[split_idx:]\n",
    "\n",
    "val_prop = 0.5\n",
    "rest_size = rest_x.shape[0]\n",
    "val_idx = int(val_prop * rest_size)\n",
    "val_x, test_x = rest_x[:val_idx], rest_x[val_idx:]\n",
    "val_y, test_y = rest_y[:val_idx], rest_y[val_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4269, 14), (5337, 14), (534, 14), (1068, 14))"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape, y.shape, val_y.shape, rest_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10 Iteration: 5 Train loss: 0.191\n",
      "Epoch: 0/10 Iteration: 10 Train loss: 0.061\n",
      "Epoch: 0/10 Iteration: 15 Train loss: 0.059\n",
      "Epoch: 0/10 Iteration: 20 Train loss: 0.060\n",
      "Epoch: 0/10 Iteration: 25 Train loss: 0.079\n",
      "Val acc: 0.929\n",
      "Epoch: 0/10 Iteration: 30 Train loss: 0.080\n",
      "Epoch: 0/10 Iteration: 35 Train loss: 0.059\n",
      "Epoch: 0/10 Iteration: 40 Train loss: 0.065\n",
      "Epoch: 1/10 Iteration: 45 Train loss: 0.050\n",
      "Epoch: 1/10 Iteration: 50 Train loss: 0.072\n",
      "Val acc: 0.929\n",
      "Epoch: 1/10 Iteration: 55 Train loss: 0.056\n",
      "Epoch: 1/10 Iteration: 60 Train loss: 0.055\n",
      "Epoch: 1/10 Iteration: 65 Train loss: 0.061\n",
      "Epoch: 1/10 Iteration: 70 Train loss: 0.072\n",
      "Epoch: 1/10 Iteration: 75 Train loss: 0.057\n",
      "Val acc: 0.929\n",
      "Epoch: 1/10 Iteration: 80 Train loss: 0.055\n",
      "Epoch: 2/10 Iteration: 85 Train loss: 0.058\n",
      "Epoch: 2/10 Iteration: 90 Train loss: 0.060\n",
      "Epoch: 2/10 Iteration: 95 Train loss: 0.056\n",
      "Epoch: 2/10 Iteration: 100 Train loss: 0.062\n",
      "Val acc: 0.929\n",
      "Epoch: 2/10 Iteration: 105 Train loss: 0.065\n",
      "Epoch: 2/10 Iteration: 110 Train loss: 0.063\n",
      "Epoch: 2/10 Iteration: 115 Train loss: 0.065\n",
      "Epoch: 2/10 Iteration: 120 Train loss: 0.058\n",
      "Epoch: 2/10 Iteration: 125 Train loss: 0.055\n",
      "Val acc: 0.929\n",
      "Epoch: 3/10 Iteration: 130 Train loss: 0.065\n",
      "Epoch: 3/10 Iteration: 135 Train loss: 0.050\n",
      "Epoch: 3/10 Iteration: 140 Train loss: 0.071\n",
      "Epoch: 3/10 Iteration: 145 Train loss: 0.052\n",
      "Epoch: 3/10 Iteration: 150 Train loss: 0.047\n",
      "Val acc: 0.929\n",
      "Epoch: 3/10 Iteration: 155 Train loss: 0.054\n",
      "Epoch: 3/10 Iteration: 160 Train loss: 0.061\n",
      "Epoch: 3/10 Iteration: 165 Train loss: 0.059\n",
      "Epoch: 4/10 Iteration: 170 Train loss: 0.063\n",
      "Epoch: 4/10 Iteration: 175 Train loss: 0.053\n",
      "Val acc: 0.929\n",
      "Epoch: 4/10 Iteration: 180 Train loss: 0.056\n",
      "Epoch: 4/10 Iteration: 185 Train loss: 0.075\n",
      "Epoch: 4/10 Iteration: 190 Train loss: 0.051\n",
      "Epoch: 4/10 Iteration: 195 Train loss: 0.051\n",
      "Epoch: 4/10 Iteration: 200 Train loss: 0.060\n",
      "Val acc: 0.929\n",
      "Epoch: 4/10 Iteration: 205 Train loss: 0.076\n",
      "Epoch: 4/10 Iteration: 210 Train loss: 0.066\n",
      "Epoch: 5/10 Iteration: 215 Train loss: 0.066\n",
      "Epoch: 5/10 Iteration: 220 Train loss: 0.054\n",
      "Epoch: 5/10 Iteration: 225 Train loss: 0.054\n",
      "Val acc: 0.929\n",
      "Epoch: 5/10 Iteration: 230 Train loss: 0.049\n",
      "Epoch: 5/10 Iteration: 235 Train loss: 0.083\n",
      "Epoch: 5/10 Iteration: 240 Train loss: 0.071\n",
      "Epoch: 5/10 Iteration: 245 Train loss: 0.053\n",
      "Epoch: 5/10 Iteration: 250 Train loss: 0.061\n",
      "Val acc: 0.929\n",
      "Epoch: 6/10 Iteration: 255 Train loss: 0.048\n",
      "Epoch: 6/10 Iteration: 260 Train loss: 0.062\n",
      "Epoch: 6/10 Iteration: 265 Train loss: 0.054\n",
      "Epoch: 6/10 Iteration: 270 Train loss: 0.055\n",
      "Epoch: 6/10 Iteration: 275 Train loss: 0.060\n",
      "Val acc: 0.929\n",
      "Epoch: 6/10 Iteration: 280 Train loss: 0.068\n",
      "Epoch: 6/10 Iteration: 285 Train loss: 0.053\n",
      "Epoch: 6/10 Iteration: 290 Train loss: 0.051\n",
      "Epoch: 7/10 Iteration: 295 Train loss: 0.057\n",
      "Epoch: 7/10 Iteration: 300 Train loss: 0.056\n",
      "Val acc: 0.929\n",
      "Epoch: 7/10 Iteration: 305 Train loss: 0.050\n",
      "Epoch: 7/10 Iteration: 310 Train loss: 0.059\n",
      "Epoch: 7/10 Iteration: 315 Train loss: 0.057\n",
      "Epoch: 7/10 Iteration: 320 Train loss: 0.063\n",
      "Epoch: 7/10 Iteration: 325 Train loss: 0.062\n",
      "Val acc: 0.929\n",
      "Epoch: 7/10 Iteration: 330 Train loss: 0.056\n",
      "Epoch: 7/10 Iteration: 335 Train loss: 0.054\n",
      "Epoch: 8/10 Iteration: 340 Train loss: 0.064\n",
      "Epoch: 8/10 Iteration: 345 Train loss: 0.050\n",
      "Epoch: 8/10 Iteration: 350 Train loss: 0.066\n",
      "Val acc: 0.929\n",
      "Epoch: 8/10 Iteration: 355 Train loss: 0.052\n",
      "Epoch: 8/10 Iteration: 360 Train loss: 0.046\n",
      "Epoch: 8/10 Iteration: 365 Train loss: 0.053\n",
      "Epoch: 8/10 Iteration: 370 Train loss: 0.059\n",
      "Epoch: 8/10 Iteration: 375 Train loss: 0.056\n",
      "Val acc: 0.929\n",
      "Epoch: 9/10 Iteration: 380 Train loss: 0.060\n",
      "Epoch: 9/10 Iteration: 385 Train loss: 0.052\n",
      "Epoch: 9/10 Iteration: 390 Train loss: 0.054\n",
      "Epoch: 9/10 Iteration: 395 Train loss: 0.074\n",
      "Epoch: 9/10 Iteration: 400 Train loss: 0.051\n",
      "Val acc: 0.929\n",
      "Epoch: 9/10 Iteration: 405 Train loss: 0.051\n",
      "Epoch: 9/10 Iteration: 410 Train loss: 0.059\n",
      "Epoch: 9/10 Iteration: 415 Train loss: 0.077\n",
      "Epoch: 9/10 Iteration: 420 Train loss: 0.062\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (batch_x, batch_y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "\n",
    "            feed = {inputs_: batch_x,\n",
    "                    labels_: batch_y,\n",
    "                    keep_prob: 0.5,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "\n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "            if iteration%25==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                \n",
    "                for batch_val_x, batch_val_y in get_batches(val_x, val_y, batch_size):\n",
    "                    feed = {inputs_: batch_val_x,\n",
    "                            labels_: batch_val_y,\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "\n",
    "            \n",
    "            iteration +=1\n",
    "            saver.save(sess, \"checkpoints/sentiment.ckpt\")\n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

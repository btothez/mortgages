{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"shuffled-full-set-hashed.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DELETION OF INTEREST</td>\n",
       "      <td>e04a09c87692 d6b72e591b91 5d066f0246f1 ed41171...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RETURNED CHECK</td>\n",
       "      <td>a3b334c6eefd be95012ebf2b 41d67080e078 ff1c26e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BILL</td>\n",
       "      <td>586242498a88 9ccf259ca087 54709b24b45f 6bf9c0c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BILL</td>\n",
       "      <td>cd50e861f48b 6ca2dd348663 d38820625542 f077614...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BILL</td>\n",
       "      <td>9db5536263d8 1c303d15eb65 3f89b4673455 b73e657...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0                                                  1\n",
       "0  DELETION OF INTEREST  e04a09c87692 d6b72e591b91 5d066f0246f1 ed41171...\n",
       "1        RETURNED CHECK  a3b334c6eefd be95012ebf2b 41d67080e078 ff1c26e...\n",
       "2                  BILL  586242498a88 9ccf259ca087 54709b24b45f 6bf9c0c...\n",
       "3                  BILL  cd50e861f48b 6ca2dd348663 d38820625542 f077614...\n",
       "4                  BILL  9db5536263d8 1c303d15eb65 3f89b4673455 b73e657..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "  \n",
    "#  Reads ‘alice.txt’ file \n",
    "sample = open(\"C:\\\\Users\\\\Admin\\\\Desktop\\\\alice.txt\", \"r\") \n",
    "s = sample.read() \n",
    "  \n",
    "# Replaces escape character with space \n",
    "f = s.replace(\"\\n\", \" \") \n",
    "  \n",
    "data = [] \n",
    "  \n",
    "# iterate through each sentence in the file \n",
    "for i in sent_tokenize(f): \n",
    "    temp = [] \n",
    "      \n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp) \n",
    "  \n",
    "# Create CBOW model \n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5) \n",
    "  \n",
    "# Print results \n",
    "print(\"Cosine similarity between 'alice' \" + \n",
    "               \"and 'wonderland' - CBOW : \", \n",
    "    model1.similarity('alice', 'wonderland')) \n",
    "      \n",
    "print(\"Cosine similarity between 'alice' \" +\n",
    "                 \"and 'machines' - CBOW : \", \n",
    "      model1.similarity('alice', 'machines')) \n",
    "  \n",
    "# # Create Skip Gram model \n",
    "# model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, \n",
    "#                                              window = 5, sg = 1) \n",
    "  \n",
    "# # Print results \n",
    "# print(\"Cosine similarity between 'alice' \" +\n",
    "#           \"and 'wonderland' - Skip Gram : \", \n",
    "#     model2.similarity('alice', 'wonderland')) \n",
    "      \n",
    "# print(\"Cosine similarity between 'alice' \" +\n",
    "#             \"and 'machines' - Skip Gram : \", \n",
    "#       model2.similarity('alice', 'machines')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/boaz.reisman/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "#  Reads ‘alice.txt’ file \n",
    "sample = open(\"ulysses.txt\", \"r\") \n",
    "s = sample.read() \n",
    "  \n",
    "# Replaces escape character with space \n",
    "f = s.replace(\"\\n\", \" \") \n",
    "\n",
    "data = [] \n",
    "  \n",
    "# iterate through each sentence in the file \n",
    "for i in sent_tokenize(f): \n",
    "    temp = [] \n",
    "      \n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp) \n",
    "  \n",
    "# Create CBOW model \n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between stately & plump 0.9082543\n",
      "Cosine similarity molly and stephen 0.9379359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Print results \n",
    "print(\"Cosine similarity between stately & plump\",\n",
    "    model1.similarity('stately', 'plump')) \n",
    "      \n",
    "print(\"Cosine similarity molly and stephen\",\n",
    "      model1.similarity('molly', 'stephen'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_seqs = [x.split(' ') for x in df[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = gensim.models.Word2Vec(list_of_seqs, min_count = 1,  \n",
    "                              size = 100, window = 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.2876782 , -1.6393709 ,  1.0918756 , -0.15285057, -0.49653086,\n",
       "       -1.7998664 ,  0.15558626,  0.32405075, -2.5706725 , -0.5051178 ,\n",
       "       -1.823094  , -0.92694944,  1.9547423 , -3.9266665 ,  0.37842792,\n",
       "       -1.6999105 , -0.83885455,  0.40298578,  0.33187824, -3.5896904 ,\n",
       "        0.48616686,  2.1415029 , -0.40368897,  0.3654191 ,  0.31358495,\n",
       "        1.6145602 , -3.2305148 , -0.36465952,  0.42663455,  1.222179  ,\n",
       "       -3.7389407 ,  0.24845056, -2.0642395 , -1.1470802 , -1.9136102 ,\n",
       "        2.5557392 , -0.8684581 , -1.5084149 ,  3.0187624 ,  0.6685917 ,\n",
       "        0.58107966, -0.12966427,  1.018735  , -2.0941586 , -2.8666434 ,\n",
       "        1.8626314 , -0.41905296,  0.05545424,  1.1254636 ,  0.07317962,\n",
       "       -2.2134936 , -0.53294307, -3.2471688 , -3.270237  , -2.1740358 ,\n",
       "       -1.787981  ,  0.14939575, -1.7104746 ,  1.0531877 , -1.7822953 ,\n",
       "       -0.6566029 ,  0.19079603, -2.55527   , -0.34516266,  0.8153599 ,\n",
       "       -2.491402  , -3.6351275 ,  1.4192287 ,  2.1528904 , -1.1547724 ,\n",
       "       -1.1400394 ,  0.1709194 ,  1.3877457 , -2.2989693 ,  1.9156992 ,\n",
       "        0.21537095, -0.14692433, -0.9938893 , -2.5581026 ,  1.4500134 ,\n",
       "        0.85324216,  2.1070037 ,  0.64374006, -1.5759536 ,  0.14849862,\n",
       "       -0.43851653,  0.74997556, -0.06325383, -0.23014316,  1.5354285 ,\n",
       "       -0.8652295 , -0.3775217 , -1.448425  ,  0.5175267 , -3.04085   ,\n",
       "        2.1654987 ,  1.6283574 , -0.38037443, -0.41101453,  0.12592542],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.wv['be95012ebf2b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.039151303"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.similarity('be95012ebf2b', '54709b24b45f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(embeddings, open('embeddings.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "df = pd.read_csv(\"shuffled-full-set-hashed.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = pickle.load(open('../server/webservice/pickles/encoder.pkl', 'rb'))\n",
    "smaller_vectorizer = pickle.load(open('./smaller_vectorizer.pkl', 'rb'))\n",
    "new_lsa = pickle.load(open('./new_lsa.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_size = len(encoder.classes_)\n",
    "sample = df[0:10000]\n",
    "sample.dropna(inplace=True)\n",
    "sample_y = sample[0]\n",
    "\n",
    "def get_label_vect(label):\n",
    "    vect = [0] * output_size\n",
    "    vect[label - 1] = 1\n",
    "    return vect\n",
    "encoded_y = np.array([get_label_vect(l) for l in encoder.transform(sample_y)])\n",
    "encoded_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 20\n",
    "def chunks(l, n):\n",
    "    # For item i in a range that is a length of l,\n",
    "    for i in range(0, len(l), n):\n",
    "        # Create an index range for l of n items:\n",
    "        new_chunk = l[i:i+n]\n",
    "        if len(new_chunk) < n:\n",
    "            new_chunk = ([''] * (n - len(new_chunk))) + new_chunk\n",
    "        yield new_chunk\n",
    "        \n",
    "doc_arr = [x for x in sample[1]]\n",
    "doc_arr[0].split(' ')\n",
    "doc_words = [doc.split(' ') for doc in doc_arr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = []\n",
    "len(doc_words), len(encoded_y)\n",
    "for i in range(len(doc_words)):\n",
    "    for chunk in chunks(doc_words[i], chunk_size):\n",
    "        training_set.append((encoded_y[i], chunk))\n",
    "        \n",
    "training_vects = []        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170380"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 170380\n",
      "500 of 170380\n",
      "1000 of 170380\n",
      "1500 of 170380\n",
      "2000 of 170380\n",
      "2500 of 170380\n",
      "3000 of 170380\n",
      "3500 of 170380\n",
      "4000 of 170380\n",
      "4500 of 170380\n",
      "5000 of 170380\n",
      "5500 of 170380\n",
      "6000 of 170380\n",
      "6500 of 170380\n",
      "7000 of 170380\n",
      "7500 of 170380\n",
      "8000 of 170380\n",
      "8500 of 170380\n",
      "9000 of 170380\n",
      "9500 of 170380\n",
      "10000 of 170380\n",
      "10500 of 170380\n",
      "11000 of 170380\n",
      "11500 of 170380\n",
      "12000 of 170380\n",
      "12500 of 170380\n",
      "13000 of 170380\n",
      "13500 of 170380\n",
      "14000 of 170380\n",
      "14500 of 170380\n",
      "15000 of 170380\n",
      "15500 of 170380\n",
      "16000 of 170380\n",
      "16500 of 170380\n",
      "17000 of 170380\n",
      "17500 of 170380\n",
      "18000 of 170380\n",
      "18500 of 170380\n",
      "19000 of 170380\n",
      "19500 of 170380\n",
      "20000 of 170380\n",
      "20500 of 170380\n",
      "21000 of 170380\n",
      "21500 of 170380\n",
      "22000 of 170380\n",
      "22500 of 170380\n",
      "23000 of 170380\n",
      "23500 of 170380\n",
      "24000 of 170380\n",
      "24500 of 170380\n",
      "25000 of 170380\n",
      "25500 of 170380\n",
      "26000 of 170380\n",
      "26500 of 170380\n",
      "27000 of 170380\n",
      "27500 of 170380\n",
      "28000 of 170380\n",
      "28500 of 170380\n",
      "29000 of 170380\n",
      "29500 of 170380\n",
      "30000 of 170380\n",
      "30500 of 170380\n",
      "31000 of 170380\n",
      "31500 of 170380\n",
      "32000 of 170380\n",
      "32500 of 170380\n",
      "33000 of 170380\n",
      "33500 of 170380\n",
      "34000 of 170380\n",
      "34500 of 170380\n",
      "35000 of 170380\n",
      "35500 of 170380\n",
      "36000 of 170380\n",
      "36500 of 170380\n",
      "37000 of 170380\n",
      "37500 of 170380\n",
      "38000 of 170380\n",
      "38500 of 170380\n",
      "39000 of 170380\n",
      "39500 of 170380\n",
      "40000 of 170380\n",
      "40500 of 170380\n",
      "41000 of 170380\n",
      "41500 of 170380\n",
      "42000 of 170380\n",
      "42500 of 170380\n",
      "43000 of 170380\n",
      "43500 of 170380\n",
      "44000 of 170380\n",
      "44500 of 170380\n",
      "45000 of 170380\n",
      "45500 of 170380\n",
      "46000 of 170380\n",
      "46500 of 170380\n",
      "47000 of 170380\n",
      "47500 of 170380\n",
      "48000 of 170380\n",
      "48500 of 170380\n",
      "49000 of 170380\n",
      "49500 of 170380\n",
      "50000 of 170380\n",
      "50500 of 170380\n",
      "51000 of 170380\n",
      "51500 of 170380\n",
      "52000 of 170380\n",
      "52500 of 170380\n",
      "53000 of 170380\n",
      "53500 of 170380\n",
      "54000 of 170380\n",
      "54500 of 170380\n",
      "55000 of 170380\n",
      "55500 of 170380\n",
      "56000 of 170380\n",
      "56500 of 170380\n",
      "57000 of 170380\n",
      "57500 of 170380\n",
      "58000 of 170380\n",
      "58500 of 170380\n",
      "59000 of 170380\n",
      "59500 of 170380\n",
      "60000 of 170380\n",
      "60500 of 170380\n",
      "61000 of 170380\n",
      "61500 of 170380\n",
      "62000 of 170380\n",
      "62500 of 170380\n",
      "63000 of 170380\n",
      "63500 of 170380\n",
      "64000 of 170380\n",
      "64500 of 170380\n",
      "65000 of 170380\n",
      "65500 of 170380\n",
      "66000 of 170380\n",
      "66500 of 170380\n",
      "67000 of 170380\n",
      "67500 of 170380\n",
      "68000 of 170380\n",
      "68500 of 170380\n",
      "69000 of 170380\n",
      "69500 of 170380\n",
      "70000 of 170380\n",
      "70500 of 170380\n",
      "71000 of 170380\n",
      "71500 of 170380\n",
      "72000 of 170380\n",
      "72500 of 170380\n",
      "73000 of 170380\n",
      "73500 of 170380\n",
      "74000 of 170380\n",
      "74500 of 170380\n",
      "75000 of 170380\n",
      "75500 of 170380\n",
      "76000 of 170380\n",
      "76500 of 170380\n",
      "77000 of 170380\n",
      "77500 of 170380\n",
      "78000 of 170380\n",
      "78500 of 170380\n",
      "79000 of 170380\n",
      "79500 of 170380\n",
      "80000 of 170380\n",
      "80500 of 170380\n",
      "81000 of 170380\n",
      "81500 of 170380\n",
      "82000 of 170380\n",
      "82500 of 170380\n",
      "83000 of 170380\n",
      "83500 of 170380\n",
      "84000 of 170380\n",
      "84500 of 170380\n",
      "85000 of 170380\n",
      "85500 of 170380\n",
      "86000 of 170380\n",
      "86500 of 170380\n",
      "87000 of 170380\n",
      "87500 of 170380\n",
      "88000 of 170380\n",
      "88500 of 170380\n",
      "89000 of 170380\n",
      "89500 of 170380\n",
      "90000 of 170380\n",
      "90500 of 170380\n",
      "91000 of 170380\n",
      "91500 of 170380\n",
      "92000 of 170380\n",
      "92500 of 170380\n",
      "93000 of 170380\n",
      "93500 of 170380\n",
      "94000 of 170380\n",
      "94500 of 170380\n",
      "95000 of 170380\n",
      "95500 of 170380\n",
      "96000 of 170380\n",
      "96500 of 170380\n",
      "97000 of 170380\n",
      "97500 of 170380\n",
      "98000 of 170380\n",
      "98500 of 170380\n",
      "99000 of 170380\n",
      "99500 of 170380\n",
      "100000 of 170380\n",
      "100500 of 170380\n",
      "101000 of 170380\n",
      "101500 of 170380\n",
      "102000 of 170380\n",
      "102500 of 170380\n",
      "103000 of 170380\n",
      "103500 of 170380\n",
      "104000 of 170380\n",
      "104500 of 170380\n",
      "105000 of 170380\n",
      "105500 of 170380\n",
      "106000 of 170380\n",
      "106500 of 170380\n",
      "107000 of 170380\n",
      "107500 of 170380\n",
      "108000 of 170380\n",
      "108500 of 170380\n",
      "109000 of 170380\n",
      "109500 of 170380\n",
      "110000 of 170380\n",
      "110500 of 170380\n",
      "111000 of 170380\n",
      "111500 of 170380\n",
      "112000 of 170380\n",
      "112500 of 170380\n",
      "113000 of 170380\n",
      "113500 of 170380\n",
      "114000 of 170380\n",
      "114500 of 170380\n",
      "115000 of 170380\n",
      "115500 of 170380\n",
      "116000 of 170380\n",
      "116500 of 170380\n",
      "117000 of 170380\n",
      "117500 of 170380\n",
      "118000 of 170380\n",
      "118500 of 170380\n",
      "119000 of 170380\n",
      "119500 of 170380\n",
      "120000 of 170380\n",
      "120500 of 170380\n",
      "121000 of 170380\n",
      "121500 of 170380\n",
      "122000 of 170380\n",
      "122500 of 170380\n",
      "123000 of 170380\n",
      "123500 of 170380\n",
      "124000 of 170380\n",
      "124500 of 170380\n",
      "125000 of 170380\n",
      "125500 of 170380\n",
      "126000 of 170380\n",
      "126500 of 170380\n",
      "127000 of 170380\n",
      "127500 of 170380\n",
      "128000 of 170380\n",
      "128500 of 170380\n",
      "129000 of 170380\n",
      "129500 of 170380\n",
      "130000 of 170380\n",
      "130500 of 170380\n",
      "131000 of 170380\n",
      "131500 of 170380\n",
      "132000 of 170380\n",
      "132500 of 170380\n",
      "133000 of 170380\n",
      "133500 of 170380\n",
      "134000 of 170380\n",
      "134500 of 170380\n",
      "135000 of 170380\n",
      "135500 of 170380\n",
      "136000 of 170380\n",
      "136500 of 170380\n",
      "137000 of 170380\n",
      "137500 of 170380\n",
      "138000 of 170380\n",
      "138500 of 170380\n",
      "139000 of 170380\n",
      "139500 of 170380\n",
      "140000 of 170380\n",
      "140500 of 170380\n",
      "141000 of 170380\n",
      "141500 of 170380\n",
      "142000 of 170380\n",
      "142500 of 170380\n",
      "143000 of 170380\n",
      "143500 of 170380\n",
      "144000 of 170380\n",
      "144500 of 170380\n",
      "145000 of 170380\n",
      "145500 of 170380\n",
      "146000 of 170380\n",
      "146500 of 170380\n",
      "147000 of 170380\n",
      "147500 of 170380\n",
      "148000 of 170380\n",
      "148500 of 170380\n",
      "149000 of 170380\n",
      "149500 of 170380\n",
      "150000 of 170380\n",
      "150500 of 170380\n",
      "151000 of 170380\n",
      "151500 of 170380\n",
      "152000 of 170380\n",
      "152500 of 170380\n",
      "153000 of 170380\n",
      "153500 of 170380\n",
      "154000 of 170380\n",
      "154500 of 170380\n",
      "155000 of 170380\n",
      "155500 of 170380\n",
      "156000 of 170380\n",
      "156500 of 170380\n",
      "157000 of 170380\n",
      "157500 of 170380\n",
      "158000 of 170380\n",
      "158500 of 170380\n",
      "159000 of 170380\n",
      "159500 of 170380\n",
      "160000 of 170380\n",
      "160500 of 170380\n",
      "161000 of 170380\n",
      "161500 of 170380\n",
      "162000 of 170380\n",
      "162500 of 170380\n",
      "163000 of 170380\n",
      "163500 of 170380\n",
      "164000 of 170380\n",
      "164500 of 170380\n",
      "165000 of 170380\n",
      "165500 of 170380\n",
      "166000 of 170380\n",
      "166500 of 170380\n",
      "167000 of 170380\n",
      "167500 of 170380\n",
      "168000 of 170380\n",
      "168500 of 170380\n",
      "169000 of 170380\n",
      "169500 of 170380\n",
      "170000 of 170380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " [array([ 0.1434493 ,  0.02206802, -0.07441233,  0.02167092,  0.00049866,\n",
       "         -0.09899638,  0.01377018, -0.00330719,  0.03428012,  0.11761893,\n",
       "         -0.1722495 ,  0.01206386, -0.0321017 , -0.07461794,  0.10068704,\n",
       "          0.12426958, -0.02082334, -0.03588277, -0.00687876,  0.0053177 ,\n",
       "          0.07970418,  0.00292972, -0.00854009, -0.08107421, -0.02387904,\n",
       "          0.07992019, -0.08593847, -0.00419079, -0.00518038,  0.00845898,\n",
       "          0.02266443, -0.04533478,  0.0849636 ,  0.06195972,  0.16432809,\n",
       "          0.03302996, -0.09877219, -0.03342144, -0.03901842,  0.04955141,\n",
       "          0.11901986, -0.11069657,  0.0300322 ,  0.03475586,  0.0606498 ,\n",
       "         -0.14606519, -0.06019993,  0.09585275, -0.02104146, -0.13177359,\n",
       "         -0.15374803, -0.06068371,  0.03552494,  0.01796003,  0.06610803,\n",
       "         -0.00524239,  0.10040445,  0.19491042,  0.13472122, -0.02049665,\n",
       "          0.15770875,  0.07092995, -0.15797417, -0.25890489,  0.12756778,\n",
       "          0.20401169, -0.02819536, -0.05821868, -0.17930811, -0.05008378,\n",
       "          0.11662905, -0.15583162,  0.00221528,  0.03786705,  0.19837293,\n",
       "          0.09836573, -0.01362808,  0.11853457, -0.27830751,  0.09763257,\n",
       "          0.20229751,  0.05092713,  0.06169737,  0.0809879 , -0.15241893,\n",
       "         -0.02620752,  0.07284407,  0.05616469,  0.02645585,  0.03171315,\n",
       "          0.1482809 ,  0.0137098 , -0.09055142,  0.0989067 ,  0.0628086 ,\n",
       "          0.10477311,  0.08480226,  0.0715857 , -0.21455365,  0.07987887]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([ 0.11364061,  0.01092876,  0.10104311,  0.03670005, -0.04069266,\n",
       "         -0.12165271, -0.13366615,  0.06926855,  0.01822636, -0.03320347,\n",
       "         -0.1148095 ,  0.09049483, -0.1301324 , -0.1011077 ,  0.16176404,\n",
       "          0.05603024, -0.14590182, -0.12943895, -0.01303281,  0.09669549,\n",
       "         -0.01232154,  0.06057291,  0.04935622,  0.06047623, -0.21772665,\n",
       "          0.00332675, -0.06531535, -0.01898441, -0.01916167, -0.05552743,\n",
       "         -0.09594395,  0.10908866, -0.03375215,  0.0078754 ,  0.23459029,\n",
       "          0.15532497,  0.01639589,  0.01387139, -0.07657781,  0.01618423,\n",
       "         -0.06090719, -0.02874052, -0.03102321,  0.16919508, -0.04100569,\n",
       "          0.1081229 , -0.16332457, -0.25134106, -0.00446301, -0.08380285,\n",
       "         -0.02431695, -0.02665519,  0.09458696, -0.05585559,  0.06809975,\n",
       "         -0.10677115,  0.02252776,  0.06194288,  0.10305282, -0.12911332,\n",
       "          0.11377164, -0.16788458, -0.00244844, -0.03988308,  0.0738704 ,\n",
       "          0.03094037, -0.09895409,  0.07273985, -0.18454691,  0.0742475 ,\n",
       "         -0.11848275,  0.107799  , -0.02999996,  0.00957837,  0.01869006,\n",
       "          0.1398747 ,  0.07902215, -0.06439186, -0.1151937 ,  0.03042553,\n",
       "         -0.15738864, -0.07506833,  0.02097242, -0.00479152, -0.03813584,\n",
       "         -0.03919461, -0.04646728,  0.04213961, -0.27298259,  0.11153421,\n",
       "          0.04084725,  0.25249619, -0.20411716,  0.02517389, -0.0469605 ,\n",
       "          0.00348444,  0.1021152 , -0.09951205, -0.01966545, -0.01476761]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([ 9.09561691e-02, -1.45120802e-02, -1.00905039e-02,  4.03418853e-02,\n",
       "          1.74057982e-02, -7.80146715e-02, -2.86724389e-02, -7.27916465e-02,\n",
       "          4.10100453e-02,  2.80173492e-02, -4.69412679e-02, -6.43509201e-02,\n",
       "          1.39882592e-02,  4.57695974e-03,  6.39676134e-03,  5.56975874e-02,\n",
       "          1.24942525e-01, -2.97972227e-02, -8.23969570e-02,  1.09450422e-01,\n",
       "          1.04941329e-01,  1.34770918e-01,  2.19192801e-02,  2.76112918e-02,\n",
       "          5.86086289e-02,  9.30441412e-03, -6.72529836e-03, -3.18015631e-02,\n",
       "         -8.64587855e-03, -1.26344036e-02,  2.41932202e-02,  3.85306389e-02,\n",
       "         -4.28513845e-02,  3.82176839e-02,  6.65748081e-02, -4.02207402e-02,\n",
       "          1.84488245e-02, -3.31408129e-02,  6.08936418e-02, -1.38542977e-01,\n",
       "          1.23593717e-01,  2.07846229e-02, -4.14092775e-04,  5.93516152e-02,\n",
       "         -6.83341389e-02, -9.02550597e-02, -2.08411826e-02,  8.48086943e-02,\n",
       "          1.80179395e-02,  1.02684187e-01,  6.02514625e-02,  1.93619601e-01,\n",
       "          1.56183017e-02,  5.10878589e-02, -3.48463729e-02,  1.35352838e-01,\n",
       "          1.44588728e-01, -2.46618547e-02,  4.18781193e-02, -7.17999064e-02,\n",
       "          5.14822814e-02,  3.13055061e-02,  3.62913760e-02, -1.39097416e-01,\n",
       "          2.09311342e-02,  1.09489038e-01, -2.30185207e-01, -9.56241336e-02,\n",
       "          7.88509540e-02,  1.03825262e-01,  2.58547477e-02,  7.52836234e-02,\n",
       "          2.64650187e-02,  3.72892879e-02, -1.66692499e-02, -5.31503509e-02,\n",
       "          1.55557295e-02,  9.49249809e-02,  1.39895221e-01,  8.16050902e-03,\n",
       "         -5.84886900e-02,  5.72590875e-02, -2.66226226e-01, -1.91894628e-01,\n",
       "          7.08601432e-03,  1.41247577e-01, -4.16486089e-01, -1.57821573e-01,\n",
       "          3.11671319e-02,  2.75704299e-02, -2.41580316e-02, -1.08463337e-02,\n",
       "          2.86319873e-01,  1.48242196e-01,  5.45691736e-02, -2.05254887e-01,\n",
       "         -1.26236723e-02,  8.60165214e-02,  1.49609313e-01,  2.14256044e-01]),\n",
       "  array([ 5.13554504e-02,  4.11992396e-03, -1.47549410e-02,  4.54964350e-02,\n",
       "          3.20945151e-02, -7.83857570e-02,  4.93076940e-02, -1.66616013e-02,\n",
       "          1.25090016e-02,  4.91735175e-02, -7.54435308e-03, -3.05039635e-02,\n",
       "          1.40286702e-02, -6.70826174e-03,  2.87532020e-02,  3.12198041e-02,\n",
       "          4.38860970e-05,  2.24485630e-02,  9.19146574e-03,  4.05708818e-02,\n",
       "          1.14684315e-01,  4.73655175e-02, -8.69597219e-03,  1.24457578e-01,\n",
       "          2.39052228e-02,  6.40633991e-02, -9.90294658e-02, -5.66768003e-02,\n",
       "         -3.70704657e-02, -2.03146806e-02,  2.68799186e-02,  4.04413951e-02,\n",
       "         -2.45059718e-02,  7.75779477e-02,  1.41063248e-01, -8.31970577e-02,\n",
       "         -3.04382611e-02, -6.94535037e-02,  8.41501705e-02, -1.10190763e-01,\n",
       "          7.06174661e-02, -4.69763099e-02,  1.95344318e-02,  3.28039402e-02,\n",
       "         -2.05042433e-03, -3.69484036e-02, -2.23679502e-02,  1.01402061e-01,\n",
       "          2.06964431e-02,  5.45306663e-02,  1.50644157e-03,  1.92681933e-01,\n",
       "         -6.43050256e-03,  1.78869487e-02, -4.11390776e-02,  7.90775186e-02,\n",
       "          1.86375461e-01, -2.12676453e-02, -1.14999910e-01, -4.23973607e-02,\n",
       "          1.65543178e-02, -2.56457204e-02,  4.78069400e-02, -9.15246285e-02,\n",
       "          4.14646159e-02,  2.98828302e-02, -1.39841800e-01, -7.50898176e-02,\n",
       "          8.45797870e-02,  1.54062822e-01, -2.08572338e-03,  5.50559108e-02,\n",
       "          7.18464393e-02,  1.35615319e-02,  2.32038621e-02,  5.62760159e-02,\n",
       "          3.46255249e-03,  1.29167875e-01,  3.18420838e-03,  6.04820166e-02,\n",
       "         -8.12066686e-02,  7.49463671e-02, -2.62352737e-01, -1.93282136e-01,\n",
       "         -2.41720268e-02,  1.51973894e-01, -4.52134499e-01, -3.22520573e-02,\n",
       "          6.61362163e-02, -5.06357258e-02,  3.26175233e-02,  1.64398988e-01,\n",
       "          3.49513561e-01,  2.23218795e-01,  1.81515953e-01, -1.05212534e-01,\n",
       "         -3.86471622e-03,  6.98579308e-03, -1.09069793e-02,  2.11585936e-01]),\n",
       "  array([ 0.0680263 , -0.00919963, -0.00055141,  0.05491943,  0.02549165,\n",
       "         -0.05958872, -0.01218759, -0.01201573,  0.00898004,  0.03754419,\n",
       "         -0.03418814, -0.01304043,  0.02132264,  0.04541638,  0.04575657,\n",
       "          0.00749981,  0.0268433 ,  0.10817041,  0.00469183, -0.02391453,\n",
       "          0.35089665,  0.09948075,  0.04882052,  0.43597771,  0.28383533,\n",
       "          0.22939638, -0.07267375, -0.13762943, -0.18808762,  0.06817958,\n",
       "          0.04754689,  0.02496876, -0.15080256,  0.12561228,  0.05205188,\n",
       "         -0.03641515,  0.06957729, -0.12003938,  0.03896487,  0.00544612,\n",
       "         -0.07741636,  0.06747564,  0.02095306,  0.07725856, -0.04059668,\n",
       "         -0.03644142, -0.03333083,  0.02968094, -0.01238164, -0.02708956,\n",
       "          0.04937987,  0.1209281 ,  0.09281792,  0.11671096,  0.01814443,\n",
       "          0.01884494,  0.00599516, -0.00321557, -0.06238992, -0.01965887,\n",
       "          0.01166421,  0.03543354,  0.03652014, -0.04345871,  0.100283  ,\n",
       "         -0.00737597, -0.05476606, -0.03729206,  0.10151505,  0.15602257,\n",
       "         -0.0458758 ,  0.03346883,  0.03588668, -0.05804053,  0.0139018 ,\n",
       "          0.07273481,  0.0139917 ,  0.05552082, -0.06529658,  0.05063882,\n",
       "          0.00307057,  0.03275919, -0.18315478, -0.15876986, -0.03802067,\n",
       "          0.14457462, -0.29202487,  0.00379612,  0.03683011, -0.00207077,\n",
       "         -0.04142899,  0.09161967,  0.15410463,  0.09834799,  0.06103624,\n",
       "         -0.01382186, -0.00064019,  0.04566325, -0.04851374,  0.1141645 ]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([ 0.17803395, -0.00373656,  0.15700288, -0.01854531,  0.13754875,\n",
       "          0.07706576,  0.02379127,  0.14195394,  0.03373859,  0.02148901,\n",
       "          0.03062063,  0.08422881, -0.05811767, -0.05880231, -0.03689893,\n",
       "         -0.24903516,  0.08323682,  0.06853325, -0.09191612, -0.07237029,\n",
       "         -0.12573213,  0.02766441, -0.03802871, -0.03710135,  0.10723946,\n",
       "          0.01723658, -0.20176245,  0.18081326, -0.04390414,  0.17177123,\n",
       "         -0.26008384,  0.10032356, -0.00196805,  0.25016105, -0.10534631,\n",
       "          0.06925311,  0.07586685,  0.2025232 , -0.05020469,  0.00781791,\n",
       "          0.09044356, -0.132359  ,  0.10467508,  0.20294511, -0.05442878,\n",
       "          0.11427712, -0.02793204,  0.17807571,  0.05447262, -0.0826647 ,\n",
       "         -0.0528783 ,  0.0373877 , -0.02694724, -0.0701667 , -0.06756986,\n",
       "          0.00676252, -0.00218247,  0.07120599,  0.01192846, -0.13660918,\n",
       "          0.07280373, -0.14535742,  0.07584203,  0.03272691,  0.09576178,\n",
       "          0.02534048,  0.05528812, -0.06309226, -0.05546978, -0.00368728,\n",
       "          0.12145978,  0.14797059,  0.05302513, -0.01903127,  0.05321356,\n",
       "         -0.03507356, -0.01466465,  0.10453181,  0.08262233, -0.04922187,\n",
       "          0.02791258,  0.1312731 ,  0.03245341, -0.11623671,  0.01320775,\n",
       "          0.045807  ,  0.04567864,  0.07434994,  0.04776318,  0.05450264,\n",
       "          0.07008918, -0.059585  ,  0.07178691, -0.02962876,  0.26185356,\n",
       "         -0.07318381,  0.01537812,  0.03963354,  0.131688  ,  0.0612735 ]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([ 0.06377434, -0.03637708, -0.04288076, -0.00814867, -0.01449356,\n",
       "          0.00620922, -0.0246383 , -0.00770313, -0.01625811,  0.01000126,\n",
       "         -0.08664003,  0.0839036 , -0.04699513, -0.03265771,  0.08119257,\n",
       "          0.0304649 ,  0.01240923, -0.00229056,  0.03630686, -0.02767323,\n",
       "          0.03708805, -0.03553669, -0.0050456 ,  0.01974918, -0.05524558,\n",
       "         -0.05743048, -0.05063401,  0.01057656, -0.06855354, -0.00936256,\n",
       "         -0.01279138, -0.12017454,  0.17318485,  0.02807855,  0.09986098,\n",
       "          0.1849483 , -0.16963221,  0.10486885,  0.2011538 ,  0.01658975,\n",
       "         -0.18303189, -0.00614921, -0.04861386,  0.14497177, -0.08626914,\n",
       "         -0.10726006, -0.08939283, -0.1515754 , -0.19462657, -0.02565719,\n",
       "          0.00588662, -0.11051757,  0.07888274, -0.06780084,  0.09325675,\n",
       "         -0.05040463,  0.08815329,  0.06461474,  0.16487779, -0.04086541,\n",
       "          0.17282808, -0.13149212,  0.02737335, -0.02879941, -0.0272235 ,\n",
       "         -0.08311257, -0.0280183 , -0.15485568, -0.09152548,  0.06073618,\n",
       "         -0.14334622, -0.03469023, -0.00504067, -0.11855459, -0.05672784,\n",
       "          0.14889712,  0.20286981, -0.01653139, -0.08549393,  0.23980694,\n",
       "         -0.20894014, -0.00772418, -0.01726096,  0.04695282, -0.11113647,\n",
       "         -0.02159636, -0.10241642,  0.02727133, -0.18761143,  0.1251322 ,\n",
       "          0.10097499, -0.00502021,  0.04773032,  0.19683221, -0.14056472,\n",
       "          0.09318647, -0.03979728, -0.15161099, -0.18450627, -0.19056438]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([ 0.06002022,  0.00748786,  0.00312784,  0.02484316,  0.00185746,\n",
       "         -0.0263855 , -0.01521128,  0.02318528,  0.02825458,  0.01849467,\n",
       "         -0.08005896,  0.07077289,  0.03724034,  0.02934366,  0.01598061,\n",
       "          0.00191673, -0.04615518, -0.00574615, -0.02357177, -0.04531104,\n",
       "         -0.02043354,  0.0277238 , -0.03424714,  0.01296564, -0.02740237,\n",
       "         -0.03188522, -0.01128273, -0.08183472, -0.02232416,  0.05356655,\n",
       "         -0.08420026,  0.17126127,  0.22794396,  0.06963395,  0.31265039,\n",
       "          0.0561708 , -0.34101281,  0.02043891, -0.29709724, -0.00976385,\n",
       "         -0.03588914,  0.02817487,  0.09118359,  0.03326758, -0.00870293,\n",
       "         -0.02876118, -0.01348055,  0.00425683,  0.03388221,  0.10734347,\n",
       "          0.20833838,  0.15610339, -0.07020663, -0.1623342 , -0.1309164 ,\n",
       "         -0.02738966, -0.09726527, -0.19469294, -0.04654238, -0.01237224,\n",
       "          0.22854313,  0.19641023,  0.09085833,  0.1645446 ,  0.06048931,\n",
       "         -0.16493652, -0.10468723, -0.04751661,  0.01198228, -0.01754479,\n",
       "          0.00365616, -0.16729479,  0.0347748 , -0.00085585, -0.00398168,\n",
       "         -0.06628212, -0.08037505, -0.04862258,  0.05652662,  0.04382967,\n",
       "          0.2626394 ,  0.0642177 ,  0.0118301 , -0.0613106 ,  0.00226191,\n",
       "          0.00601022, -0.0600332 , -0.02580091, -0.0866336 , -0.09155567,\n",
       "          0.13772379, -0.06849204, -0.16080412,  0.02117758, -0.01916001,\n",
       "          0.01987051, -0.02295736,  0.00916122, -0.03388655,  0.02991154]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now for each word, get the tfidf vector\n",
    "def word_2_vect(word):\n",
    "    return new_lsa.transform(smaller_vectorizer.transform(pd.Series([word])))[0]\n",
    "\n",
    "i = 0\n",
    "for label, chunk in training_set:\n",
    "    training_vects.append((label, [word_2_vect(word) for word in chunk]))\n",
    "    if i%500 == 0:\n",
    "        print('{} of {}'.format(i, len(training_set)))\n",
    "    i += 1\n",
    "\n",
    "training_vects[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170380"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(training_vects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for ch in chunks(training_vects, 80000):\n",
    "    pickle.dump(ch, open('training_vects_big_{}.pkl'.format(i), 'wb'))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "lstm_layers = 2\n",
    "batch_size = 500\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "# Add nodes to the graph\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(\"float\", [None, chunk_size, 100])\n",
    "    labels_ = tf.placeholder(tf.int32, [None, output_size])\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1003 20:04:28.423163 140736238351232 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W1003 20:04:28.424947 140736238351232 deprecation.py:323] From <ipython-input-16-ab0169ed8749>:5: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W1003 20:04:28.431388 140736238351232 deprecation.py:323] From <ipython-input-16-ab0169ed8749>:10: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    with tf.name_scope(\"RNN_layers\"):\n",
    "        def lstm_cell():\n",
    "            # Your basic LSTM cell\n",
    "            lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size, reuse=tf.get_variable_scope().reuse)\n",
    "            # Add dropout to the cell\n",
    "            return tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "\n",
    "        # Stack up multiple LSTM layers, for deep learning\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(lstm_layers)])\n",
    "\n",
    "        # Getting an initial state of all zeros\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1003 20:04:34.272701 140736238351232 deprecation.py:323] From <ipython-input-17-94a791623b59>:3: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W1003 20:04:34.592230 140736238351232 deprecation.py:506] From /Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1003 20:04:34.602071 140736238351232 deprecation.py:506] From /Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs_,\n",
    "                                             initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1003 20:04:43.100605 140736238351232 deprecation.py:323] From /Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], output_size, activation_fn=tf.sigmoid)\n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170380, 14)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([tup[0] for tup in training_vects])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170380, 20, 100)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([tup[1] for tup in training_vects])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=300):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prop = 0.8\n",
    "\n",
    "set_size = x.shape[0]\n",
    "\n",
    "split_idx = int(set_size*0.8)\n",
    "train_x, rest_x = x[:split_idx], x[split_idx:]\n",
    "train_y, rest_y = y[:split_idx], y[split_idx:]\n",
    "\n",
    "val_prop = 0.5\n",
    "rest_size = rest_x.shape[0]\n",
    "val_idx = int(val_prop * rest_size)\n",
    "val_x, test_x = rest_x[:val_idx], rest_x[val_idx:]\n",
    "val_y, test_y = rest_y[:val_idx], rest_y[val_idx:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((136304, 14), (170380, 14), (17038, 14), (34076, 14))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape, y.shape, val_y.shape, rest_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10 Iteration: 5 Train loss: 0.195\n",
      "Epoch: 0/10 Iteration: 10 Train loss: 0.061\n",
      "Epoch: 0/10 Iteration: 15 Train loss: 0.062\n",
      "Epoch: 0/10 Iteration: 20 Train loss: 0.061\n",
      "Epoch: 0/10 Iteration: 25 Train loss: 0.058\n",
      "Val acc: 0.929\n",
      "Epoch: 0/10 Iteration: 30 Train loss: 0.060\n",
      "Epoch: 0/10 Iteration: 35 Train loss: 0.060\n",
      "Epoch: 0/10 Iteration: 40 Train loss: 0.058\n",
      "Epoch: 0/10 Iteration: 45 Train loss: 0.057\n",
      "Epoch: 0/10 Iteration: 50 Train loss: 0.059\n",
      "Val acc: 0.929\n",
      "Epoch: 0/10 Iteration: 55 Train loss: 0.057\n",
      "Epoch: 0/10 Iteration: 60 Train loss: 0.057\n",
      "Epoch: 0/10 Iteration: 65 Train loss: 0.057\n",
      "Epoch: 0/10 Iteration: 70 Train loss: 0.058\n",
      "Epoch: 0/10 Iteration: 75 Train loss: 0.058\n",
      "Val acc: 0.929\n",
      "Epoch: 0/10 Iteration: 80 Train loss: 0.058\n",
      "Epoch: 0/10 Iteration: 85 Train loss: 0.057\n",
      "Epoch: 0/10 Iteration: 90 Train loss: 0.057\n",
      "Epoch: 0/10 Iteration: 95 Train loss: 0.056\n",
      "Epoch: 0/10 Iteration: 100 Train loss: 0.057\n",
      "Val acc: 0.929\n",
      "Epoch: 0/10 Iteration: 105 Train loss: 0.057\n",
      "Epoch: 0/10 Iteration: 110 Train loss: 0.055\n",
      "Epoch: 0/10 Iteration: 115 Train loss: 0.057\n",
      "Epoch: 0/10 Iteration: 120 Train loss: 0.057\n",
      "Epoch: 0/10 Iteration: 125 Train loss: 0.057\n",
      "Val acc: 0.929\n",
      "Epoch: 0/10 Iteration: 130 Train loss: 0.057\n",
      "Epoch: 0/10 Iteration: 135 Train loss: 0.056\n",
      "Epoch: 0/10 Iteration: 140 Train loss: 0.056\n",
      "Epoch: 0/10 Iteration: 145 Train loss: 0.055\n",
      "Epoch: 0/10 Iteration: 150 Train loss: 0.056\n",
      "Val acc: 0.929\n",
      "Epoch: 0/10 Iteration: 155 Train loss: 0.057\n",
      "Epoch: 0/10 Iteration: 160 Train loss: 0.056\n",
      "Epoch: 0/10 Iteration: 165 Train loss: 0.057\n",
      "Epoch: 0/10 Iteration: 170 Train loss: 0.056\n",
      "Epoch: 0/10 Iteration: 175 Train loss: 0.056\n",
      "Val acc: 0.929\n",
      "Epoch: 0/10 Iteration: 180 Train loss: 0.056\n",
      "Epoch: 0/10 Iteration: 185 Train loss: 0.057\n",
      "Epoch: 0/10 Iteration: 190 Train loss: 0.058\n",
      "Epoch: 0/10 Iteration: 195 Train loss: 0.057\n",
      "Epoch: 0/10 Iteration: 200 Train loss: 0.057\n",
      "Val acc: 0.929\n",
      "Epoch: 0/10 Iteration: 205 Train loss: 0.056\n",
      "Epoch: 0/10 Iteration: 210 Train loss: 0.055\n",
      "Epoch: 0/10 Iteration: 215 Train loss: 0.056\n",
      "Epoch: 0/10 Iteration: 220 Train loss: 0.056\n",
      "Epoch: 0/10 Iteration: 225 Train loss: 0.057\n",
      "Val acc: 0.929\n",
      "Epoch: 0/10 Iteration: 230 Train loss: 0.055\n",
      "Epoch: 0/10 Iteration: 235 Train loss: 0.058\n",
      "Epoch: 0/10 Iteration: 240 Train loss: 0.056\n",
      "Epoch: 0/10 Iteration: 245 Train loss: 0.056\n",
      "Epoch: 0/10 Iteration: 250 Train loss: 0.056\n",
      "Val acc: 0.929\n",
      "Epoch: 0/10 Iteration: 255 Train loss: 0.056\n",
      "Epoch: 0/10 Iteration: 260 Train loss: 0.057\n",
      "Epoch: 0/10 Iteration: 265 Train loss: 0.055\n",
      "Epoch: 0/10 Iteration: 270 Train loss: 0.055\n",
      "Epoch: 1/10 Iteration: 275 Train loss: 0.057\n",
      "Val acc: 0.929\n",
      "Epoch: 1/10 Iteration: 280 Train loss: 0.056\n",
      "Epoch: 1/10 Iteration: 285 Train loss: 0.056\n",
      "Epoch: 1/10 Iteration: 290 Train loss: 0.056\n",
      "Epoch: 1/10 Iteration: 295 Train loss: 0.055\n",
      "Epoch: 1/10 Iteration: 300 Train loss: 0.056\n",
      "Val acc: 0.931\n",
      "Epoch: 1/10 Iteration: 305 Train loss: 0.056\n",
      "Epoch: 1/10 Iteration: 310 Train loss: 0.055\n",
      "Epoch: 1/10 Iteration: 315 Train loss: 0.057\n",
      "Epoch: 1/10 Iteration: 320 Train loss: 0.056\n",
      "Epoch: 1/10 Iteration: 325 Train loss: 0.051\n",
      "Val acc: 0.914\n",
      "Epoch: 1/10 Iteration: 330 Train loss: 0.054\n",
      "Epoch: 1/10 Iteration: 335 Train loss: 0.056\n",
      "Epoch: 1/10 Iteration: 340 Train loss: 0.056\n",
      "Epoch: 1/10 Iteration: 345 Train loss: 0.056\n",
      "Epoch: 1/10 Iteration: 350 Train loss: 0.054\n",
      "Val acc: 0.929\n",
      "Epoch: 1/10 Iteration: 355 Train loss: 0.055\n",
      "Epoch: 1/10 Iteration: 360 Train loss: 0.052\n",
      "Epoch: 1/10 Iteration: 365 Train loss: 0.050\n",
      "Epoch: 1/10 Iteration: 370 Train loss: 0.052\n",
      "Epoch: 1/10 Iteration: 375 Train loss: 0.052\n",
      "Val acc: 0.935\n",
      "Epoch: 1/10 Iteration: 380 Train loss: 0.053\n",
      "Epoch: 1/10 Iteration: 385 Train loss: 0.053\n",
      "Epoch: 1/10 Iteration: 390 Train loss: 0.052\n",
      "Epoch: 1/10 Iteration: 395 Train loss: 0.049\n",
      "Epoch: 1/10 Iteration: 400 Train loss: 0.047\n",
      "Val acc: 0.935\n",
      "Epoch: 1/10 Iteration: 405 Train loss: 0.048\n",
      "Epoch: 1/10 Iteration: 410 Train loss: 0.052\n",
      "Epoch: 1/10 Iteration: 415 Train loss: 0.048\n",
      "Epoch: 1/10 Iteration: 420 Train loss: 0.049\n",
      "Epoch: 1/10 Iteration: 425 Train loss: 0.048\n",
      "Val acc: 0.939\n",
      "Epoch: 1/10 Iteration: 430 Train loss: 0.048\n",
      "Epoch: 1/10 Iteration: 435 Train loss: 0.048\n",
      "Epoch: 1/10 Iteration: 440 Train loss: 0.047\n",
      "Epoch: 1/10 Iteration: 445 Train loss: 0.046\n",
      "Epoch: 1/10 Iteration: 450 Train loss: 0.045\n",
      "Val acc: 0.941\n",
      "Epoch: 1/10 Iteration: 455 Train loss: 0.049\n",
      "Epoch: 1/10 Iteration: 460 Train loss: 0.046\n",
      "Epoch: 1/10 Iteration: 465 Train loss: 0.046\n",
      "Epoch: 1/10 Iteration: 470 Train loss: 0.048\n",
      "Epoch: 1/10 Iteration: 475 Train loss: 0.049\n",
      "Val acc: 0.942\n",
      "Epoch: 1/10 Iteration: 480 Train loss: 0.048\n",
      "Epoch: 1/10 Iteration: 485 Train loss: 0.047\n",
      "Epoch: 1/10 Iteration: 490 Train loss: 0.045\n",
      "Epoch: 1/10 Iteration: 495 Train loss: 0.046\n",
      "Epoch: 1/10 Iteration: 500 Train loss: 0.047\n",
      "Val acc: 0.942\n",
      "Epoch: 1/10 Iteration: 505 Train loss: 0.046\n",
      "Epoch: 1/10 Iteration: 510 Train loss: 0.044\n",
      "Epoch: 1/10 Iteration: 515 Train loss: 0.046\n",
      "Epoch: 1/10 Iteration: 520 Train loss: 0.043\n",
      "Epoch: 1/10 Iteration: 525 Train loss: 0.044\n",
      "Val acc: 0.944\n",
      "Epoch: 1/10 Iteration: 530 Train loss: 0.046\n",
      "Epoch: 1/10 Iteration: 535 Train loss: 0.045\n",
      "Epoch: 1/10 Iteration: 540 Train loss: 0.046\n",
      "Epoch: 2/10 Iteration: 545 Train loss: 0.045\n",
      "Epoch: 2/10 Iteration: 550 Train loss: 0.047\n",
      "Val acc: 0.944\n",
      "Epoch: 2/10 Iteration: 555 Train loss: 0.046\n",
      "Epoch: 2/10 Iteration: 560 Train loss: 0.047\n",
      "Epoch: 2/10 Iteration: 565 Train loss: 0.046\n",
      "Epoch: 2/10 Iteration: 570 Train loss: 0.045\n",
      "Epoch: 2/10 Iteration: 575 Train loss: 0.046\n",
      "Val acc: 0.945\n",
      "Epoch: 2/10 Iteration: 580 Train loss: 0.042\n",
      "Epoch: 2/10 Iteration: 585 Train loss: 0.047\n",
      "Epoch: 2/10 Iteration: 590 Train loss: 0.044\n",
      "Epoch: 2/10 Iteration: 595 Train loss: 0.044\n",
      "Epoch: 2/10 Iteration: 600 Train loss: 0.046\n",
      "Val acc: 0.943\n",
      "Epoch: 2/10 Iteration: 605 Train loss: 0.048\n",
      "Epoch: 2/10 Iteration: 610 Train loss: 0.046\n",
      "Epoch: 2/10 Iteration: 615 Train loss: 0.048\n",
      "Epoch: 2/10 Iteration: 620 Train loss: 0.042\n",
      "Epoch: 2/10 Iteration: 625 Train loss: 0.046\n",
      "Val acc: 0.944\n",
      "Epoch: 2/10 Iteration: 630 Train loss: 0.041\n",
      "Epoch: 2/10 Iteration: 635 Train loss: 0.040\n",
      "Epoch: 2/10 Iteration: 640 Train loss: 0.045\n",
      "Epoch: 2/10 Iteration: 645 Train loss: 0.044\n",
      "Epoch: 2/10 Iteration: 650 Train loss: 0.046\n",
      "Val acc: 0.946\n",
      "Epoch: 2/10 Iteration: 655 Train loss: 0.044\n",
      "Epoch: 2/10 Iteration: 660 Train loss: 0.044\n",
      "Epoch: 2/10 Iteration: 665 Train loss: 0.046\n",
      "Epoch: 2/10 Iteration: 670 Train loss: 0.046\n",
      "Epoch: 2/10 Iteration: 675 Train loss: 0.043\n",
      "Val acc: 0.947\n",
      "Epoch: 2/10 Iteration: 680 Train loss: 0.043\n",
      "Epoch: 2/10 Iteration: 685 Train loss: 0.042\n",
      "Epoch: 2/10 Iteration: 690 Train loss: 0.043\n",
      "Epoch: 2/10 Iteration: 695 Train loss: 0.045\n",
      "Epoch: 2/10 Iteration: 700 Train loss: 0.043\n",
      "Val acc: 0.946\n",
      "Epoch: 2/10 Iteration: 705 Train loss: 0.040\n",
      "Epoch: 2/10 Iteration: 710 Train loss: 0.041\n",
      "Epoch: 2/10 Iteration: 715 Train loss: 0.042\n",
      "Epoch: 2/10 Iteration: 720 Train loss: 0.042\n",
      "Epoch: 2/10 Iteration: 725 Train loss: 0.044\n",
      "Val acc: 0.947\n",
      "Epoch: 2/10 Iteration: 730 Train loss: 0.041\n",
      "Epoch: 2/10 Iteration: 735 Train loss: 0.044\n",
      "Epoch: 2/10 Iteration: 740 Train loss: 0.043\n",
      "Epoch: 2/10 Iteration: 745 Train loss: 0.042\n",
      "Epoch: 2/10 Iteration: 750 Train loss: 0.040\n",
      "Val acc: 0.947\n",
      "Epoch: 2/10 Iteration: 755 Train loss: 0.041\n",
      "Epoch: 2/10 Iteration: 760 Train loss: 0.043\n",
      "Epoch: 2/10 Iteration: 765 Train loss: 0.042\n",
      "Epoch: 2/10 Iteration: 770 Train loss: 0.041\n",
      "Epoch: 2/10 Iteration: 775 Train loss: 0.043\n",
      "Val acc: 0.948\n",
      "Epoch: 2/10 Iteration: 780 Train loss: 0.042\n",
      "Epoch: 2/10 Iteration: 785 Train loss: 0.045\n",
      "Epoch: 2/10 Iteration: 790 Train loss: 0.043\n",
      "Epoch: 2/10 Iteration: 795 Train loss: 0.042\n",
      "Epoch: 2/10 Iteration: 800 Train loss: 0.044\n",
      "Val acc: 0.948\n",
      "Epoch: 2/10 Iteration: 805 Train loss: 0.039\n",
      "Epoch: 2/10 Iteration: 810 Train loss: 0.041\n",
      "Epoch: 2/10 Iteration: 815 Train loss: 0.043\n",
      "Epoch: 3/10 Iteration: 820 Train loss: 0.041\n",
      "Epoch: 3/10 Iteration: 825 Train loss: 0.042\n",
      "Val acc: 0.948\n",
      "Epoch: 3/10 Iteration: 830 Train loss: 0.044\n",
      "Epoch: 3/10 Iteration: 835 Train loss: 0.042\n",
      "Epoch: 3/10 Iteration: 840 Train loss: 0.042\n",
      "Epoch: 3/10 Iteration: 845 Train loss: 0.041\n",
      "Epoch: 3/10 Iteration: 850 Train loss: 0.042\n",
      "Val acc: 0.948\n",
      "Epoch: 3/10 Iteration: 855 Train loss: 0.042\n",
      "Epoch: 3/10 Iteration: 860 Train loss: 0.043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/10 Iteration: 865 Train loss: 0.045\n",
      "Epoch: 3/10 Iteration: 870 Train loss: 0.041\n",
      "Epoch: 3/10 Iteration: 875 Train loss: 0.041\n",
      "Val acc: 0.947\n",
      "Epoch: 3/10 Iteration: 880 Train loss: 0.043\n",
      "Epoch: 3/10 Iteration: 885 Train loss: 0.043\n",
      "Epoch: 3/10 Iteration: 890 Train loss: 0.040\n",
      "Epoch: 3/10 Iteration: 895 Train loss: 0.040\n",
      "Epoch: 3/10 Iteration: 900 Train loss: 0.042\n",
      "Val acc: 0.948\n",
      "Epoch: 3/10 Iteration: 905 Train loss: 0.041\n",
      "Epoch: 3/10 Iteration: 910 Train loss: 0.043\n",
      "Epoch: 3/10 Iteration: 915 Train loss: 0.040\n",
      "Epoch: 3/10 Iteration: 920 Train loss: 0.039\n",
      "Epoch: 3/10 Iteration: 925 Train loss: 0.041\n",
      "Val acc: 0.948\n",
      "Epoch: 3/10 Iteration: 930 Train loss: 0.041\n",
      "Epoch: 3/10 Iteration: 935 Train loss: 0.041\n",
      "Epoch: 3/10 Iteration: 940 Train loss: 0.039\n",
      "Epoch: 3/10 Iteration: 945 Train loss: 0.042\n",
      "Epoch: 3/10 Iteration: 950 Train loss: 0.043\n",
      "Val acc: 0.949\n",
      "Epoch: 3/10 Iteration: 955 Train loss: 0.040\n",
      "Epoch: 3/10 Iteration: 960 Train loss: 0.040\n",
      "Epoch: 3/10 Iteration: 965 Train loss: 0.042\n",
      "Epoch: 3/10 Iteration: 970 Train loss: 0.041\n",
      "Epoch: 3/10 Iteration: 975 Train loss: 0.043\n",
      "Val acc: 0.950\n",
      "Epoch: 3/10 Iteration: 980 Train loss: 0.043\n",
      "Epoch: 3/10 Iteration: 985 Train loss: 0.041\n",
      "Epoch: 3/10 Iteration: 990 Train loss: 0.041\n",
      "Epoch: 3/10 Iteration: 995 Train loss: 0.039\n",
      "Epoch: 3/10 Iteration: 1000 Train loss: 0.042\n",
      "Val acc: 0.949\n",
      "Epoch: 3/10 Iteration: 1005 Train loss: 0.042\n",
      "Epoch: 3/10 Iteration: 1010 Train loss: 0.039\n",
      "Epoch: 3/10 Iteration: 1015 Train loss: 0.041\n",
      "Epoch: 3/10 Iteration: 1020 Train loss: 0.037\n",
      "Epoch: 3/10 Iteration: 1025 Train loss: 0.040\n",
      "Val acc: 0.949\n",
      "Epoch: 3/10 Iteration: 1030 Train loss: 0.041\n",
      "Epoch: 3/10 Iteration: 1035 Train loss: 0.040\n",
      "Epoch: 3/10 Iteration: 1040 Train loss: 0.041\n",
      "Epoch: 3/10 Iteration: 1045 Train loss: 0.037\n",
      "Epoch: 3/10 Iteration: 1050 Train loss: 0.040\n",
      "Val acc: 0.950\n",
      "Epoch: 3/10 Iteration: 1055 Train loss: 0.040\n",
      "Epoch: 3/10 Iteration: 1060 Train loss: 0.041\n",
      "Epoch: 3/10 Iteration: 1065 Train loss: 0.038\n",
      "Epoch: 3/10 Iteration: 1070 Train loss: 0.037\n",
      "Epoch: 3/10 Iteration: 1075 Train loss: 0.041\n",
      "Val acc: 0.950\n",
      "Epoch: 3/10 Iteration: 1080 Train loss: 0.041\n",
      "Epoch: 3/10 Iteration: 1085 Train loss: 0.038\n",
      "Epoch: 4/10 Iteration: 1090 Train loss: 0.041\n",
      "Epoch: 4/10 Iteration: 1095 Train loss: 0.041\n",
      "Epoch: 4/10 Iteration: 1100 Train loss: 0.041\n",
      "Val acc: 0.947\n",
      "Epoch: 4/10 Iteration: 1105 Train loss: 0.039\n",
      "Epoch: 4/10 Iteration: 1110 Train loss: 0.041\n",
      "Epoch: 4/10 Iteration: 1115 Train loss: 0.040\n",
      "Epoch: 4/10 Iteration: 1120 Train loss: 0.042\n",
      "Epoch: 4/10 Iteration: 1125 Train loss: 0.043\n",
      "Val acc: 0.947\n",
      "Epoch: 4/10 Iteration: 1130 Train loss: 0.041\n",
      "Epoch: 4/10 Iteration: 1135 Train loss: 0.042\n",
      "Epoch: 4/10 Iteration: 1140 Train loss: 0.041\n",
      "Epoch: 4/10 Iteration: 1145 Train loss: 0.040\n",
      "Epoch: 4/10 Iteration: 1150 Train loss: 0.040\n",
      "Val acc: 0.947\n",
      "Epoch: 4/10 Iteration: 1155 Train loss: 0.040\n",
      "Epoch: 4/10 Iteration: 1160 Train loss: 0.041\n",
      "Epoch: 4/10 Iteration: 1165 Train loss: 0.038\n",
      "Epoch: 4/10 Iteration: 1170 Train loss: 0.041\n",
      "Epoch: 4/10 Iteration: 1175 Train loss: 0.041\n",
      "Val acc: 0.948\n",
      "Epoch: 4/10 Iteration: 1180 Train loss: 0.041\n",
      "Epoch: 4/10 Iteration: 1185 Train loss: 0.040\n",
      "Epoch: 4/10 Iteration: 1190 Train loss: 0.039\n",
      "Epoch: 4/10 Iteration: 1195 Train loss: 0.036\n",
      "Epoch: 4/10 Iteration: 1200 Train loss: 0.037\n",
      "Val acc: 0.946\n",
      "Epoch: 4/10 Iteration: 1205 Train loss: 0.043\n",
      "Epoch: 4/10 Iteration: 1210 Train loss: 0.039\n",
      "Epoch: 4/10 Iteration: 1215 Train loss: 0.039\n",
      "Epoch: 4/10 Iteration: 1220 Train loss: 0.040\n",
      "Epoch: 4/10 Iteration: 1225 Train loss: 0.040\n",
      "Val acc: 0.947\n",
      "Epoch: 4/10 Iteration: 1230 Train loss: 0.043\n",
      "Epoch: 4/10 Iteration: 1235 Train loss: 0.039\n",
      "Epoch: 4/10 Iteration: 1240 Train loss: 0.039\n",
      "Epoch: 4/10 Iteration: 1245 Train loss: 0.037\n",
      "Epoch: 4/10 Iteration: 1250 Train loss: 0.041\n",
      "Val acc: 0.945\n",
      "Epoch: 4/10 Iteration: 1255 Train loss: 0.040\n",
      "Epoch: 4/10 Iteration: 1260 Train loss: 0.036\n",
      "Epoch: 4/10 Iteration: 1265 Train loss: 0.037\n",
      "Epoch: 4/10 Iteration: 1270 Train loss: 0.040\n",
      "Epoch: 4/10 Iteration: 1275 Train loss: 0.038\n",
      "Val acc: 0.942\n",
      "Epoch: 4/10 Iteration: 1280 Train loss: 0.042\n",
      "Epoch: 4/10 Iteration: 1285 Train loss: 0.040\n",
      "Epoch: 4/10 Iteration: 1290 Train loss: 0.036\n",
      "Epoch: 4/10 Iteration: 1295 Train loss: 0.037\n",
      "Epoch: 4/10 Iteration: 1300 Train loss: 0.039\n",
      "Val acc: 0.944\n",
      "Epoch: 4/10 Iteration: 1305 Train loss: 0.039\n",
      "Epoch: 4/10 Iteration: 1310 Train loss: 0.039\n",
      "Epoch: 4/10 Iteration: 1315 Train loss: 0.040\n",
      "Epoch: 4/10 Iteration: 1320 Train loss: 0.038\n",
      "Epoch: 4/10 Iteration: 1325 Train loss: 0.039\n",
      "Val acc: 0.941\n",
      "Epoch: 4/10 Iteration: 1330 Train loss: 0.039\n",
      "Epoch: 4/10 Iteration: 1335 Train loss: 0.038\n",
      "Epoch: 4/10 Iteration: 1340 Train loss: 0.039\n",
      "Epoch: 4/10 Iteration: 1345 Train loss: 0.039\n",
      "Epoch: 4/10 Iteration: 1350 Train loss: 0.039\n",
      "Val acc: 0.940\n",
      "Epoch: 4/10 Iteration: 1355 Train loss: 0.039\n",
      "Epoch: 4/10 Iteration: 1360 Train loss: 0.039\n",
      "Epoch: 5/10 Iteration: 1365 Train loss: 0.047\n",
      "Epoch: 5/10 Iteration: 1370 Train loss: 0.039\n",
      "Epoch: 5/10 Iteration: 1375 Train loss: 0.039\n",
      "Val acc: 0.951\n",
      "Epoch: 5/10 Iteration: 1380 Train loss: 0.038\n",
      "Epoch: 5/10 Iteration: 1385 Train loss: 0.037\n",
      "Epoch: 5/10 Iteration: 1390 Train loss: 0.041\n",
      "Epoch: 5/10 Iteration: 1395 Train loss: 0.039\n",
      "Epoch: 5/10 Iteration: 1400 Train loss: 0.039\n",
      "Val acc: 0.950\n",
      "Epoch: 5/10 Iteration: 1405 Train loss: 0.037\n",
      "Epoch: 5/10 Iteration: 1410 Train loss: 0.041\n",
      "Epoch: 5/10 Iteration: 1415 Train loss: 0.038\n",
      "Epoch: 5/10 Iteration: 1420 Train loss: 0.040\n",
      "Epoch: 5/10 Iteration: 1425 Train loss: 0.038\n",
      "Val acc: 0.951\n",
      "Epoch: 5/10 Iteration: 1430 Train loss: 0.038\n",
      "Epoch: 5/10 Iteration: 1435 Train loss: 0.039\n",
      "Epoch: 5/10 Iteration: 1440 Train loss: 0.039\n",
      "Epoch: 5/10 Iteration: 1445 Train loss: 0.035\n",
      "Epoch: 5/10 Iteration: 1450 Train loss: 0.037\n",
      "Val acc: 0.951\n",
      "Epoch: 5/10 Iteration: 1455 Train loss: 0.037\n",
      "Epoch: 5/10 Iteration: 1460 Train loss: 0.038\n",
      "Epoch: 5/10 Iteration: 1465 Train loss: 0.039\n",
      "Epoch: 5/10 Iteration: 1470 Train loss: 0.039\n",
      "Epoch: 5/10 Iteration: 1475 Train loss: 0.039\n",
      "Val acc: 0.951\n",
      "Epoch: 5/10 Iteration: 1480 Train loss: 0.040\n",
      "Epoch: 5/10 Iteration: 1485 Train loss: 0.036\n",
      "Epoch: 5/10 Iteration: 1490 Train loss: 0.036\n",
      "Epoch: 5/10 Iteration: 1495 Train loss: 0.041\n",
      "Epoch: 5/10 Iteration: 1500 Train loss: 0.036\n",
      "Val acc: 0.951\n",
      "Epoch: 5/10 Iteration: 1505 Train loss: 0.039\n",
      "Epoch: 5/10 Iteration: 1510 Train loss: 0.039\n",
      "Epoch: 5/10 Iteration: 1515 Train loss: 0.040\n",
      "Epoch: 5/10 Iteration: 1520 Train loss: 0.035\n",
      "Epoch: 5/10 Iteration: 1525 Train loss: 0.039\n",
      "Val acc: 0.951\n",
      "Epoch: 5/10 Iteration: 1530 Train loss: 0.036\n",
      "Epoch: 5/10 Iteration: 1535 Train loss: 0.038\n",
      "Epoch: 5/10 Iteration: 1540 Train loss: 0.039\n",
      "Epoch: 5/10 Iteration: 1545 Train loss: 0.037\n",
      "Epoch: 5/10 Iteration: 1550 Train loss: 0.042\n",
      "Val acc: 0.951\n",
      "Epoch: 5/10 Iteration: 1555 Train loss: 0.037\n",
      "Epoch: 5/10 Iteration: 1560 Train loss: 0.039\n",
      "Epoch: 5/10 Iteration: 1565 Train loss: 0.036\n",
      "Epoch: 5/10 Iteration: 1570 Train loss: 0.035\n",
      "Epoch: 5/10 Iteration: 1575 Train loss: 0.039\n",
      "Val acc: 0.951\n",
      "Epoch: 5/10 Iteration: 1580 Train loss: 0.039\n",
      "Epoch: 5/10 Iteration: 1585 Train loss: 0.039\n",
      "Epoch: 5/10 Iteration: 1590 Train loss: 0.035\n",
      "Epoch: 5/10 Iteration: 1595 Train loss: 0.039\n",
      "Epoch: 5/10 Iteration: 1600 Train loss: 0.038\n",
      "Val acc: 0.950\n",
      "Epoch: 5/10 Iteration: 1605 Train loss: 0.038\n",
      "Epoch: 5/10 Iteration: 1610 Train loss: 0.039\n",
      "Epoch: 5/10 Iteration: 1615 Train loss: 0.037\n",
      "Epoch: 5/10 Iteration: 1620 Train loss: 0.038\n",
      "Epoch: 5/10 Iteration: 1625 Train loss: 0.039\n",
      "Val acc: 0.951\n",
      "Epoch: 5/10 Iteration: 1630 Train loss: 0.036\n",
      "Epoch: 6/10 Iteration: 1635 Train loss: 0.038\n",
      "Epoch: 6/10 Iteration: 1640 Train loss: 0.038\n",
      "Epoch: 6/10 Iteration: 1645 Train loss: 0.040\n",
      "Epoch: 6/10 Iteration: 1650 Train loss: 0.039\n",
      "Val acc: 0.952\n",
      "Epoch: 6/10 Iteration: 1655 Train loss: 0.036\n",
      "Epoch: 6/10 Iteration: 1660 Train loss: 0.039\n",
      "Epoch: 6/10 Iteration: 1665 Train loss: 0.038\n",
      "Epoch: 6/10 Iteration: 1670 Train loss: 0.037\n",
      "Epoch: 6/10 Iteration: 1675 Train loss: 0.039\n",
      "Val acc: 0.951\n",
      "Epoch: 6/10 Iteration: 1680 Train loss: 0.042\n",
      "Epoch: 6/10 Iteration: 1685 Train loss: 0.035\n",
      "Epoch: 6/10 Iteration: 1690 Train loss: 0.036\n",
      "Epoch: 6/10 Iteration: 1695 Train loss: 0.038\n",
      "Epoch: 6/10 Iteration: 1700 Train loss: 0.037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.952\n",
      "Epoch: 6/10 Iteration: 1705 Train loss: 0.037\n",
      "Epoch: 6/10 Iteration: 1710 Train loss: 0.036\n",
      "Epoch: 6/10 Iteration: 1715 Train loss: 0.037\n",
      "Epoch: 6/10 Iteration: 1720 Train loss: 0.037\n",
      "Epoch: 6/10 Iteration: 1725 Train loss: 0.037\n",
      "Val acc: 0.952\n",
      "Epoch: 6/10 Iteration: 1730 Train loss: 0.037\n",
      "Epoch: 6/10 Iteration: 1735 Train loss: 0.036\n",
      "Epoch: 6/10 Iteration: 1740 Train loss: 0.040\n",
      "Epoch: 6/10 Iteration: 1745 Train loss: 0.037\n",
      "Epoch: 6/10 Iteration: 1750 Train loss: 0.036\n",
      "Val acc: 0.952\n",
      "Epoch: 6/10 Iteration: 1755 Train loss: 0.034\n",
      "Epoch: 6/10 Iteration: 1760 Train loss: 0.038\n",
      "Epoch: 6/10 Iteration: 1765 Train loss: 0.038\n",
      "Epoch: 6/10 Iteration: 1770 Train loss: 0.041\n",
      "Epoch: 6/10 Iteration: 1775 Train loss: 0.036\n",
      "Val acc: 0.952\n",
      "Epoch: 6/10 Iteration: 1780 Train loss: 0.040\n",
      "Epoch: 6/10 Iteration: 1785 Train loss: 0.040\n",
      "Epoch: 6/10 Iteration: 1790 Train loss: 0.036\n",
      "Epoch: 6/10 Iteration: 1795 Train loss: 0.039\n",
      "Epoch: 6/10 Iteration: 1800 Train loss: 0.038\n",
      "Val acc: 0.952\n",
      "Epoch: 6/10 Iteration: 1805 Train loss: 0.038\n",
      "Epoch: 6/10 Iteration: 1810 Train loss: 0.035\n",
      "Epoch: 6/10 Iteration: 1815 Train loss: 0.038\n",
      "Epoch: 6/10 Iteration: 1820 Train loss: 0.035\n",
      "Epoch: 6/10 Iteration: 1825 Train loss: 0.036\n",
      "Val acc: 0.951\n",
      "Epoch: 6/10 Iteration: 1830 Train loss: 0.038\n",
      "Epoch: 6/10 Iteration: 1835 Train loss: 0.039\n",
      "Epoch: 6/10 Iteration: 1840 Train loss: 0.038\n",
      "Epoch: 6/10 Iteration: 1845 Train loss: 0.039\n",
      "Epoch: 6/10 Iteration: 1850 Train loss: 0.037\n",
      "Val acc: 0.952\n",
      "Epoch: 6/10 Iteration: 1855 Train loss: 0.039\n",
      "Epoch: 6/10 Iteration: 1860 Train loss: 0.040\n",
      "Epoch: 6/10 Iteration: 1865 Train loss: 0.038\n",
      "Epoch: 6/10 Iteration: 1870 Train loss: 0.036\n",
      "Epoch: 6/10 Iteration: 1875 Train loss: 0.037\n",
      "Val acc: 0.952\n",
      "Epoch: 6/10 Iteration: 1880 Train loss: 0.035\n",
      "Epoch: 6/10 Iteration: 1885 Train loss: 0.037\n",
      "Epoch: 6/10 Iteration: 1890 Train loss: 0.037\n",
      "Epoch: 6/10 Iteration: 1895 Train loss: 0.038\n",
      "Epoch: 6/10 Iteration: 1900 Train loss: 0.038\n",
      "Val acc: 0.952\n",
      "Epoch: 7/10 Iteration: 1905 Train loss: 0.035\n",
      "Epoch: 7/10 Iteration: 1910 Train loss: 0.039\n",
      "Epoch: 7/10 Iteration: 1915 Train loss: 0.038\n",
      "Epoch: 7/10 Iteration: 1920 Train loss: 0.038\n",
      "Epoch: 7/10 Iteration: 1925 Train loss: 0.037\n",
      "Val acc: 0.952\n",
      "Epoch: 7/10 Iteration: 1930 Train loss: 0.038\n",
      "Epoch: 7/10 Iteration: 1935 Train loss: 0.039\n",
      "Epoch: 7/10 Iteration: 1940 Train loss: 0.034\n",
      "Epoch: 7/10 Iteration: 1945 Train loss: 0.039\n",
      "Epoch: 7/10 Iteration: 1950 Train loss: 0.038\n",
      "Val acc: 0.951\n",
      "Epoch: 7/10 Iteration: 1955 Train loss: 0.035\n",
      "Epoch: 7/10 Iteration: 1960 Train loss: 0.038\n",
      "Epoch: 7/10 Iteration: 1965 Train loss: 0.039\n",
      "Epoch: 7/10 Iteration: 1970 Train loss: 0.039\n",
      "Epoch: 7/10 Iteration: 1975 Train loss: 0.040\n",
      "Val acc: 0.952\n",
      "Epoch: 7/10 Iteration: 1980 Train loss: 0.036\n",
      "Epoch: 7/10 Iteration: 1985 Train loss: 0.036\n",
      "Epoch: 7/10 Iteration: 1990 Train loss: 0.034\n",
      "Epoch: 7/10 Iteration: 1995 Train loss: 0.035\n",
      "Epoch: 7/10 Iteration: 2000 Train loss: 0.038\n",
      "Val acc: 0.953\n",
      "Epoch: 7/10 Iteration: 2005 Train loss: 0.039\n",
      "Epoch: 7/10 Iteration: 2010 Train loss: 0.039\n",
      "Epoch: 7/10 Iteration: 2015 Train loss: 0.038\n",
      "Epoch: 7/10 Iteration: 2020 Train loss: 0.036\n",
      "Epoch: 7/10 Iteration: 2025 Train loss: 0.040\n",
      "Val acc: 0.952\n",
      "Epoch: 7/10 Iteration: 2030 Train loss: 0.039\n",
      "Epoch: 7/10 Iteration: 2035 Train loss: 0.037\n",
      "Epoch: 7/10 Iteration: 2040 Train loss: 0.037\n",
      "Epoch: 7/10 Iteration: 2045 Train loss: 0.036\n",
      "Epoch: 7/10 Iteration: 2050 Train loss: 0.038\n",
      "Val acc: 0.952\n",
      "Epoch: 7/10 Iteration: 2055 Train loss: 0.039\n",
      "Epoch: 7/10 Iteration: 2060 Train loss: 0.036\n",
      "Epoch: 7/10 Iteration: 2065 Train loss: 0.034\n",
      "Epoch: 7/10 Iteration: 2070 Train loss: 0.036\n",
      "Epoch: 7/10 Iteration: 2075 Train loss: 0.037\n",
      "Val acc: 0.952\n",
      "Epoch: 7/10 Iteration: 2080 Train loss: 0.034\n",
      "Epoch: 7/10 Iteration: 2085 Train loss: 0.039\n",
      "Epoch: 7/10 Iteration: 2090 Train loss: 0.037\n",
      "Epoch: 7/10 Iteration: 2095 Train loss: 0.039\n",
      "Epoch: 7/10 Iteration: 2100 Train loss: 0.036\n",
      "Val acc: 0.953\n",
      "Epoch: 7/10 Iteration: 2105 Train loss: 0.037\n",
      "Epoch: 7/10 Iteration: 2110 Train loss: 0.035\n",
      "Epoch: 7/10 Iteration: 2115 Train loss: 0.034\n",
      "Epoch: 7/10 Iteration: 2120 Train loss: 0.039\n",
      "Epoch: 7/10 Iteration: 2125 Train loss: 0.036\n",
      "Val acc: 0.952\n",
      "Epoch: 7/10 Iteration: 2130 Train loss: 0.037\n",
      "Epoch: 7/10 Iteration: 2135 Train loss: 0.037\n",
      "Epoch: 7/10 Iteration: 2140 Train loss: 0.038\n",
      "Epoch: 7/10 Iteration: 2145 Train loss: 0.040\n",
      "Epoch: 7/10 Iteration: 2150 Train loss: 0.038\n",
      "Val acc: 0.952\n",
      "Epoch: 7/10 Iteration: 2155 Train loss: 0.038\n",
      "Epoch: 7/10 Iteration: 2160 Train loss: 0.039\n",
      "Epoch: 7/10 Iteration: 2165 Train loss: 0.035\n",
      "Epoch: 7/10 Iteration: 2170 Train loss: 0.037\n",
      "Epoch: 7/10 Iteration: 2175 Train loss: 0.039\n",
      "Val acc: 0.952\n",
      "Epoch: 8/10 Iteration: 2180 Train loss: 0.037\n",
      "Epoch: 8/10 Iteration: 2185 Train loss: 0.037\n",
      "Epoch: 8/10 Iteration: 2190 Train loss: 0.039\n",
      "Epoch: 8/10 Iteration: 2195 Train loss: 0.038\n",
      "Epoch: 8/10 Iteration: 2200 Train loss: 0.036\n",
      "Val acc: 0.953\n",
      "Epoch: 8/10 Iteration: 2205 Train loss: 0.036\n",
      "Epoch: 8/10 Iteration: 2210 Train loss: 0.036\n",
      "Epoch: 8/10 Iteration: 2215 Train loss: 0.038\n",
      "Epoch: 8/10 Iteration: 2220 Train loss: 0.041\n",
      "Epoch: 8/10 Iteration: 2225 Train loss: 0.040\n",
      "Val acc: 0.951\n",
      "Epoch: 8/10 Iteration: 2230 Train loss: 0.036\n",
      "Epoch: 8/10 Iteration: 2235 Train loss: 0.037\n",
      "Epoch: 8/10 Iteration: 2240 Train loss: 0.040\n",
      "Epoch: 8/10 Iteration: 2245 Train loss: 0.039\n",
      "Epoch: 8/10 Iteration: 2250 Train loss: 0.035\n",
      "Val acc: 0.952\n",
      "Epoch: 8/10 Iteration: 2255 Train loss: 0.037\n",
      "Epoch: 8/10 Iteration: 2260 Train loss: 0.038\n",
      "Epoch: 8/10 Iteration: 2265 Train loss: 0.037\n",
      "Epoch: 8/10 Iteration: 2270 Train loss: 0.038\n",
      "Epoch: 8/10 Iteration: 2275 Train loss: 0.037\n",
      "Val acc: 0.953\n",
      "Epoch: 8/10 Iteration: 2280 Train loss: 0.036\n",
      "Epoch: 8/10 Iteration: 2285 Train loss: 0.038\n",
      "Epoch: 8/10 Iteration: 2290 Train loss: 0.039\n",
      "Epoch: 8/10 Iteration: 2295 Train loss: 0.038\n",
      "Epoch: 8/10 Iteration: 2300 Train loss: 0.035\n",
      "Val acc: 0.953\n",
      "Epoch: 8/10 Iteration: 2305 Train loss: 0.037\n",
      "Epoch: 8/10 Iteration: 2310 Train loss: 0.038\n",
      "Epoch: 8/10 Iteration: 2315 Train loss: 0.035\n",
      "Epoch: 8/10 Iteration: 2320 Train loss: 0.036\n",
      "Epoch: 8/10 Iteration: 2325 Train loss: 0.038\n",
      "Val acc: 0.952\n",
      "Epoch: 8/10 Iteration: 2330 Train loss: 0.036\n",
      "Epoch: 8/10 Iteration: 2335 Train loss: 0.038\n",
      "Epoch: 8/10 Iteration: 2340 Train loss: 0.038\n",
      "Epoch: 8/10 Iteration: 2345 Train loss: 0.036\n",
      "Epoch: 8/10 Iteration: 2350 Train loss: 0.038\n",
      "Val acc: 0.953\n",
      "Epoch: 8/10 Iteration: 2355 Train loss: 0.036\n",
      "Epoch: 8/10 Iteration: 2360 Train loss: 0.039\n",
      "Epoch: 8/10 Iteration: 2365 Train loss: 0.039\n",
      "Epoch: 8/10 Iteration: 2370 Train loss: 0.036\n",
      "Epoch: 8/10 Iteration: 2375 Train loss: 0.038\n",
      "Val acc: 0.953\n",
      "Epoch: 8/10 Iteration: 2380 Train loss: 0.034\n",
      "Epoch: 8/10 Iteration: 2385 Train loss: 0.037\n",
      "Epoch: 8/10 Iteration: 2390 Train loss: 0.038\n",
      "Epoch: 8/10 Iteration: 2395 Train loss: 0.035\n",
      "Epoch: 8/10 Iteration: 2400 Train loss: 0.037\n",
      "Val acc: 0.953\n",
      "Epoch: 8/10 Iteration: 2405 Train loss: 0.034\n",
      "Epoch: 8/10 Iteration: 2410 Train loss: 0.037\n",
      "Epoch: 8/10 Iteration: 2415 Train loss: 0.038\n",
      "Epoch: 8/10 Iteration: 2420 Train loss: 0.038\n",
      "Epoch: 8/10 Iteration: 2425 Train loss: 0.034\n",
      "Val acc: 0.952\n",
      "Epoch: 8/10 Iteration: 2430 Train loss: 0.035\n",
      "Epoch: 8/10 Iteration: 2435 Train loss: 0.038\n",
      "Epoch: 8/10 Iteration: 2440 Train loss: 0.039\n",
      "Epoch: 8/10 Iteration: 2445 Train loss: 0.035\n",
      "Epoch: 9/10 Iteration: 2450 Train loss: 0.037\n",
      "Val acc: 0.953\n",
      "Epoch: 9/10 Iteration: 2455 Train loss: 0.037\n",
      "Epoch: 9/10 Iteration: 2460 Train loss: 0.036\n",
      "Epoch: 9/10 Iteration: 2465 Train loss: 0.035\n",
      "Epoch: 9/10 Iteration: 2470 Train loss: 0.037\n",
      "Epoch: 9/10 Iteration: 2475 Train loss: 0.036\n",
      "Val acc: 0.953\n",
      "Epoch: 9/10 Iteration: 2480 Train loss: 0.038\n",
      "Epoch: 9/10 Iteration: 2485 Train loss: 0.039\n",
      "Epoch: 9/10 Iteration: 2490 Train loss: 0.039\n",
      "Epoch: 9/10 Iteration: 2495 Train loss: 0.037\n",
      "Epoch: 9/10 Iteration: 2500 Train loss: 0.038\n",
      "Val acc: 0.953\n",
      "Epoch: 9/10 Iteration: 2505 Train loss: 0.036\n",
      "Epoch: 9/10 Iteration: 2510 Train loss: 0.036\n",
      "Epoch: 9/10 Iteration: 2515 Train loss: 0.037\n",
      "Epoch: 9/10 Iteration: 2520 Train loss: 0.039\n",
      "Epoch: 9/10 Iteration: 2525 Train loss: 0.035\n",
      "Val acc: 0.953\n",
      "Epoch: 9/10 Iteration: 2530 Train loss: 0.037\n",
      "Epoch: 9/10 Iteration: 2535 Train loss: 0.039\n",
      "Epoch: 9/10 Iteration: 2540 Train loss: 0.039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/10 Iteration: 2545 Train loss: 0.037\n",
      "Epoch: 9/10 Iteration: 2550 Train loss: 0.036\n",
      "Val acc: 0.953\n",
      "Epoch: 9/10 Iteration: 2555 Train loss: 0.035\n",
      "Epoch: 9/10 Iteration: 2560 Train loss: 0.034\n",
      "Epoch: 9/10 Iteration: 2565 Train loss: 0.041\n",
      "Epoch: 9/10 Iteration: 2570 Train loss: 0.036\n",
      "Epoch: 9/10 Iteration: 2575 Train loss: 0.038\n",
      "Val acc: 0.953\n",
      "Epoch: 9/10 Iteration: 2580 Train loss: 0.038\n",
      "Epoch: 9/10 Iteration: 2585 Train loss: 0.038\n",
      "Epoch: 9/10 Iteration: 2590 Train loss: 0.040\n",
      "Epoch: 9/10 Iteration: 2595 Train loss: 0.036\n",
      "Epoch: 9/10 Iteration: 2600 Train loss: 0.037\n",
      "Val acc: 0.953\n",
      "Epoch: 9/10 Iteration: 2605 Train loss: 0.034\n",
      "Epoch: 9/10 Iteration: 2610 Train loss: 0.036\n",
      "Epoch: 9/10 Iteration: 2615 Train loss: 0.038\n",
      "Epoch: 9/10 Iteration: 2620 Train loss: 0.033\n",
      "Epoch: 9/10 Iteration: 2625 Train loss: 0.035\n",
      "Val acc: 0.954\n",
      "Epoch: 9/10 Iteration: 2630 Train loss: 0.038\n",
      "Epoch: 9/10 Iteration: 2635 Train loss: 0.035\n",
      "Epoch: 9/10 Iteration: 2640 Train loss: 0.038\n",
      "Epoch: 9/10 Iteration: 2645 Train loss: 0.036\n",
      "Epoch: 9/10 Iteration: 2650 Train loss: 0.034\n",
      "Val acc: 0.953\n",
      "Epoch: 9/10 Iteration: 2655 Train loss: 0.035\n",
      "Epoch: 9/10 Iteration: 2660 Train loss: 0.038\n",
      "Epoch: 9/10 Iteration: 2665 Train loss: 0.036\n",
      "Epoch: 9/10 Iteration: 2670 Train loss: 0.037\n",
      "Epoch: 9/10 Iteration: 2675 Train loss: 0.037\n",
      "Val acc: 0.953\n",
      "Epoch: 9/10 Iteration: 2680 Train loss: 0.036\n",
      "Epoch: 9/10 Iteration: 2685 Train loss: 0.037\n",
      "Epoch: 9/10 Iteration: 2690 Train loss: 0.037\n",
      "Epoch: 9/10 Iteration: 2695 Train loss: 0.037\n",
      "Epoch: 9/10 Iteration: 2700 Train loss: 0.038\n",
      "Val acc: 0.953\n",
      "Epoch: 9/10 Iteration: 2705 Train loss: 0.036\n",
      "Epoch: 9/10 Iteration: 2710 Train loss: 0.036\n",
      "Epoch: 9/10 Iteration: 2715 Train loss: 0.036\n",
      "Epoch: 9/10 Iteration: 2720 Train loss: 0.036\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (batch_x, batch_y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "\n",
    "            feed = {inputs_: batch_x,\n",
    "                    labels_: batch_y,\n",
    "                    keep_prob: 0.5,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "\n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "            if iteration%25==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                \n",
    "                for batch_val_x, batch_val_y in get_batches(val_x, val_y, batch_size):\n",
    "                    feed = {inputs_: batch_val_x,\n",
    "                            labels_: batch_val_y,\n",
    "                            keep_prob: 1, \n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "\n",
    "            \n",
    "            iteration +=1\n",
    "            saver.save(sess, \"checkpoints/final_sentiment.ckpt\")\n",
    "    saver.save(sess, \"checkpoints/final_sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.954\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, \"checkpoints/final_sentiment.ckpt\")\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (bat_test_x, bat_test_y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        feed = {inputs_: bat_test_x,\n",
    "                labels_: bat_test_y,\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[ True  True False  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True\n",
      "2\n",
      "[ True False  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "False\n",
      "3\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True\n",
      "4\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True\n",
      "5\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True\n",
      "6\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True\n",
      "7\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True\n",
      "8\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True\n",
      "9\n",
      "[ True  True  True False  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "False\n",
      "10\n",
      "[ True  True  True  True  True  True  True  True  True  True False  True\n",
      "  True  True]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "True\n",
      "11\n",
      "[False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True\n",
      "12\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True\n",
      "13\n",
      "[ True  True  True  True  True  True  True  True  True  True False  True\n",
      "  True  True]\n",
      "[0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "False\n",
      "14\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "True\n",
      "15\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True\n",
      "16\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "True\n",
      "17\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True\n",
      "18\n",
      "[ True  True False  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "False\n",
      "19\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True\n",
      "20\n",
      "[False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "False\n",
      "21\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True\n",
      "22\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "True\n",
      "23\n",
      "[ True  True  True  True  True  True  True  True  True  True False  True\n",
      "  True  True]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "False\n",
      "24\n",
      "[ True  True  True  True  True  True  True  True  True  True False  True\n",
      "  True  True]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "False\n",
      "25\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True\n",
      "26\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True\n",
      "27\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True\n",
      "28\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True\n",
      "29\n",
      "[ True  True  True False  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "False\n",
      "30\n",
      "[False  True  True  True  True  True  True  True  True False  True  True\n",
      "  True  True]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "False\n",
      "31\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True\n",
      "32\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "True\n",
      "33\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "True\n",
      "34\n",
      "[False  True  True  True  True  True  True  True  True  True False  True\n",
      "  True  True]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "False\n",
      "Test accuracy: 0.954\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, \"checkpoints/final_sentiment.ckpt\")\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (bat_test_x, bat_test_y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        feed = {inputs_: bat_test_x,\n",
    "                labels_: bat_test_y,\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        batch_acc, test_state, pdctns, cor_p = sess.run([accuracy, final_state, predictions, correct_pred], feed_dict=feed)\n",
    "        print(ii)\n",
    "        print(cor_p[0])\n",
    "        print(max_prediction(pdctns))\n",
    "        print(bat_test_y[0])\n",
    "        print(all(max_prediction(pdctns) == bat_test_y[0]))\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20, 100)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = 'cd50e861f48b 6ca2dd348663 d38820625542 f07761418345 5948001254b3 9997460cf372 3f7a70f3474f 6bc122aa4b06 b4f1b88bd3f3 25c57acdf805 54709b24b45f 51fc8b56e14f ff714b9fa7fd 798fe9915030 2d5d7bc2df91 6b583fe13e39 8e93a2273a93 3d877a3bc4f0 ff1c26ea0b6f d9ef68daef4c 9a42ead47d1c 5cde1ed4f9b0 a20e78c41cf9 dec88250479b ecee69844b4e 87b8193a0183 0c470ee92bb5 cd50f04925dd 878460b4304e b4f1b88bd3f3 133d46f7ed38 d38820625542 f682089485c3 96776b42c1d5 51fc8b56e14f 3f7a70f3474f d911e9441d32 c16651ffa1b7 9374c105ef84 f8b0c07e306c d38820625542 e0a08df8ec4c f8dab246f73f 878460b4304e 3f7a70f3474f b4f1b88bd3f3 6ce6cc5a3203 b4f1b88bd3f3 ddf4525e90e3 55b85f1ee56e c63534ebfa82 878460b4304e 9448e1c70dec 1932d6ce3497 8243aca146b2 2ea49cf89745 ff714b9fa7fd e4dad7cb07b6 ff714b9fa7fd 549330b9e320 f7bb594ff117 e63fa74d3c8b df03d288b8b6 cd50e861f48b f7bb594ff117 a1bb6b4223d9 ba02159e05b1 37ac79620fc6 1087de55f102 80948b43ec70 d9fc2cff7e21 abca9d18fae2 6cfef272bdb9 9d83e581af4b cc9e05bc2a86 cbd0a3abbc34 69954839bb65 cd50f04925dd 431392fb12c7 ecee69844b4e 36e7aa72ffe1 0b3f3d23bd37 8cb71bb0ee27 ff714b9fa7fd a87473e878c1 a1bb6b4223d9 f7bb594ff117 dff2d27792d5 ff714b9fa7fd 83f8b75d8fe3 22f8fa324aed 58b1c79289d7 5948001254b3 9997460cf372 da9ad7407226 a263b63bc282 011113964d37 da9ad7407226 b73e657498f2 17c11ac4749f ee8125ecd585 0cbca93be301 ea95010f229b b13cff832743 cf4fc632eed2 b4f1b88bd3f3 6b343f522f78 4ea48dbbe14e 5f9c2ac954be 61e151399c9d 7b477e3d63d2 cdc95c39cac8 f07761418345 22fa1184be26 d9ef68daef4c d944e076fa92 f7ae6f8257da 87b8193a0183 11269ab19093 6cfef272bdb9 8ebb4fffd292 d38820625542 6469b7eeb029 98b0900e06f4 1b6c95839a6d 44c1e122f3f0 d38820625542 cfd22ba194a9 828e5c947b2b 7860028b1d17 6dbd5ceef3d1 f60bd1bc4fe5 05b8cb495fc2 9d83e581af4b d38820625542 7d9e333a86da 6cfef272bdb9 9d83e581af4b 133d46f7ed38 1087de55f102 ef4ba44cdf5f b02eb907dd1a 35991d8609e2 29f6e5802d4f 133d46f7ed38 874b796362b6 e943e5e5b779 4d28878de1db d1c3631d621b 7d9e333a86da d38820625542 133d46f7ed38 1b7657be8986 1ab34730c1e0 f7bb594ff117 ff714b9fa7fd 8502c0087099 58b1c79289d7 cd50f04925dd ecee69844b4e 431392fb12c7 1b3e5dc39eb9 586242498a88 5680bf4cc121 69c87281a156 c8207fafe699 cd50f04925dd ff714b9fa7fd 8502c0087099 ecee69844b4e 8cb71bb0ee27 386e8b20726f a1bb6b4223d9 6bf9c0cb01b4 142d16db61dd 60d18132df81 dab42b5e7528 259218088611 135307dba198 b73e657498f2 26f7353edc2e cd50f04925dd d38820625542 133d46f7ed38 f7bb594ff117 ecee69844b4e 1ab34730c1e0 e943e5e5b779 a87473e878c1 57d5616683d8 54fb27d07530 afab4a5c0f58 83da9eb0a417 619c02c2888a 11269ab19093 fb82195dee42 b7a0f56f6ce8 2d5d7bc2df91 6b583fe13e39 798fe9915030 586242498a88 0662d94b3d3b b7ab56536ec4 f54a40468e55 14bc74b659ac 7498a1e9d9d7 ac624879ac84 6bc122aa4b06 868759570f1a 8d21095e8690 4cc4edfe0691 a28d2053ff0a abca9d18fae2 586242498a88 1446e5509927 7ec02e30a5b3 00c51a5afedc b9699ce57810 ba02159e05b1 f0666bdbc8a5 e7e059c82399 6bc963b8a2b5 6d3eb69fcdd2 ae28415398b8 25c57acdf805 bce6be3921fd 77af4ed951a7 6dcc75eaf823 f9b20c280980 1b6d0614f2c7 3486e5fe0d73 4b62ac958854 1f72b4630c4a 05c859b5a17c b2f316884260 93c0dec8ced7 fc25f79e6d18 c1a112035695 1d8b1029729c c33b5c3d0449 a4ffd27183aa bf064c332aa1 62a63953f966 034e2d7f187e 0562c756a2f2 40cb5e209b92 1b6d0614f2c7 ce1f034abb5d 7420f0cec354 e943e5e5b779 b208ae1e8232 f8b0c07e306c 0072efdaa34a 98d0d51b397c fbb5efbcc5b3 4b62ac958854 e170f65889d6 33de92ab3817 4129ea7e3fb2 0c4ce226d9fe d38820625542 ba02159e05b1 cde4f1b2a877 6ce6cc5a3203 62189de225a6 f95d0bea231b 6ca2dd348663 63fa15c4caa9 c9a53ea6e219 10e45001c2f2 094453b4e4ae 586242498a88 586242498a88 9690f01b4172 818a7ff3bf29 578830762b27 cc429363fb23 e27acd17313e eeb86a6a04e4 8d21095e8690 b73e657498f2 be6b97be2959 59c7f349c89a 93790ade6682 a65259ff0092 4357c81e10c1 a31962fbd5f3 0562c756a2f2 e259a56993f4 1b6d0614f2c7 b9699ce57810 eeb86a6a04e4 b61f1af56200 036087ac04f9 25c57acdf805 b136f6349cf3 586242498a88 54709b24b45f 2d5d7bc2df91 6b583fe13e39 798fe9915030 8e93a2273a93 1c303d15eb65 bce6be3921fd 798eee3a8bec 7d41ca882f26 93c988b67c47 2ef7c27a5df4 fea862065b74 0c222c6660f2 34186de5c52d 25c57acdf805 bb0e7ae8fdbf dafbb201715e fb2cd24a447a 6bf9c0cb01b4 0562c756a2f2 ba02159e05b1 6b304aabdcee 6d25574664d2 9cdf4a63deb0 b59e343416f7 26f7353edc2e 133d46f7ed38 9bc96abb24e5 eeb86a6a04e4 f95d0bea231b 74c7b4b972ff 10e45001c2f2 10e45001c2f2 6ce6cc5a3203 10e45001c2f2 b73e657498f2 f95d0bea231b d19b1c129f40 25c57acdf805 6101ed18e42f'\n",
    "label = 'BILL'\n",
    "doc_arr = [x for x in sample[1]]\n",
    "\n",
    "word_arr = words.split(' ')\n",
    "test_word_vecs = np.array([[word_2_vect(word) for word in word_arr][:20]])\n",
    "#b_size_input = np.array([test_word_vecs[0] * 500])\n",
    "b_size_input = np.array([test_word_vecs[0] for x in range(500)])\n",
    "test_word_vecs.shape\n",
    "# (1, 20, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on MultiRNNCell in module tensorflow.python.ops.rnn_cell_impl object:\n",
      "\n",
      "class MultiRNNCell(RNNCell)\n",
      " |  RNN cell composed sequentially of multiple simple cells.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  ```python\n",
      " |  num_units = [128, 64]\n",
      " |  cells = [BasicLSTMCell(num_units=n) for n in num_units]\n",
      " |  stacked_rnn_cell = MultiRNNCell(cells)\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MultiRNNCell\n",
      " |      RNNCell\n",
      " |      tensorflow.python.layers.base.Layer\n",
      " |      tensorflow.python.keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, cells, state_is_tuple=True)\n",
      " |      Create a RNN cell composed sequentially of a number of RNNCells. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      " |      \n",
      " |      Args:\n",
      " |        cells: list of RNNCells that will be composed in this order.\n",
      " |        state_is_tuple: If True, accepted and returned states are n-tuples, where\n",
      " |          `n = len(cells)`.  If False, the states are all concatenated along the\n",
      " |          column axis.  This latter behavior will soon be deprecated.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if cells is empty (not allowed), or at least one of the cells\n",
      " |          returns a state tuple but the flag `state_is_tuple` is `False`.\n",
      " |  \n",
      " |  call(self, inputs, state)\n",
      " |      Run this multi-layer cell on inputs, starting from state.\n",
      " |  \n",
      " |  zero_state(self, batch_size, dtype)\n",
      " |      Return zero-filled state tensor(s).\n",
      " |      \n",
      " |      Args:\n",
      " |        batch_size: int, float, or unit Tensor representing the batch size.\n",
      " |        dtype: the data type to use for the state.\n",
      " |      \n",
      " |      Returns:\n",
      " |        If `state_size` is an int or TensorShape, then the return value is a\n",
      " |        `N-D` tensor of shape `[batch_size, state_size]` filled with zeros.\n",
      " |      \n",
      " |        If `state_size` is a nested list or tuple, then the return value is\n",
      " |        a nested list or tuple (of the same structure) of `2-D` tensors with\n",
      " |        the shapes `[batch_size, s]` for each s in `state_size`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  output_size\n",
      " |      Integer or TensorShape: size of outputs produced by this cell.\n",
      " |  \n",
      " |  state_size\n",
      " |      size(s) of state(s) used by this cell.\n",
      " |      \n",
      " |      It can be represented by an Integer, a TensorShape or a tuple of Integers\n",
      " |      or TensorShapes.\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from RNNCell:\n",
      " |  \n",
      " |  __call__(self, inputs, state, scope=None)\n",
      " |      Run this RNN cell on inputs, starting from the given state.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: `2-D` tensor with shape `[batch_size, input_size]`.\n",
      " |        state: if `self.state_size` is an integer, this should be a `2-D Tensor`\n",
      " |          with shape `[batch_size, self.state_size]`.  Otherwise, if\n",
      " |          `self.state_size` is a tuple of integers, this should be a tuple with\n",
      " |          shapes `[batch_size, s] for s in self.state_size`.\n",
      " |        scope: VariableScope for the created subgraph; defaults to class name.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A pair containing:\n",
      " |      \n",
      " |        - Output: A `2-D` tensor with shape `[batch_size, self.output_size]`.\n",
      " |        - New state: Either a single `2-D` tensor, or a tuple of tensors matching\n",
      " |          the arity and shapes of `state`.\n",
      " |  \n",
      " |  build(self, _)\n",
      " |      Creates the variables of the layer (optional, for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  get_initial_state(self, inputs=None, batch_size=None, dtype=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.layers.base.Layer:\n",
      " |  \n",
      " |  __deepcopy__(self, memo)\n",
      " |  \n",
      " |  __setattr__(self, value, name)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(inputs, self):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)), inputs=True)\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Actvity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(x.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      The `get_losses_for` method allows to retrieve the losses relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        inputs: Ignored when executing eagerly. If anything other than None is\n",
      " |          passed, it signals the losses are conditional on some of the layer's\n",
      " |          inputs, and thus they should only be run where these inputs are\n",
      " |          available. This is the case for activity regularization losses, for\n",
      " |          instance. If `None` is passed, the losses are assumed\n",
      " |          to be unconditional, and will apply across all dataflows of the layer\n",
      " |          (e.g. weight regularization losses).\n",
      " |  \n",
      " |  add_weight(self, name, shape, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, partitioner=None, **kwargs)\n",
      " |      Adds a new variable to the layer, or gets an existing one; returns it.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        name: variable name.\n",
      " |        shape: variable shape.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
      " |        initializer: initializer instance (callable).\n",
      " |        regularizer: regularizer instance (callable).\n",
      " |        trainable: whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean, stddev).\n",
      " |          Note, if the current variable scope is marked as non-trainable\n",
      " |          then this parameter is ignored and any added variables are also\n",
      " |          marked as non-trainable. `trainable` defaults to `True` unless\n",
      " |          `synchronization` is set to `ON_READ`.\n",
      " |        constraint: constraint instance (callable).\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        partitioner: (optional) partitioner instance (callable).  If\n",
      " |          provided, when the requested variable is created it will be split\n",
      " |          into multiple partitions according to `partitioner`.  In this case,\n",
      " |          an instance of `PartitionedVariable` is returned.  Available\n",
      " |          partitioners include `tf.compat.v1.fixed_size_partitioner` and\n",
      " |          `tf.compat.v1.variable_axis_size_partitioner`.  For more details, see\n",
      " |          the documentation of `tf.compat.v1.get_variable` and the  \"Variable\n",
      " |          Partitioners and Sharding\" section of the API guide.\n",
      " |        **kwargs: Additional keyword arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The created variable.  Usually either a `Variable` or `ResourceVariable`\n",
      " |        instance.  If `partitioner` is not `None`, a `PartitionedVariable`\n",
      " |        instance is returned.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called with partioned variable regularization and\n",
      " |          eager execution is enabled.\n",
      " |        ValueError: When trainable has been set to True with synchronization\n",
      " |          set as `ON_READ`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.layers.base.Layer:\n",
      " |  \n",
      " |  graph\n",
      " |      DEPRECATED FUNCTION\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Stop using this property because tf.layers layers no longer track their graph.\n",
      " |  \n",
      " |  scope_name\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  add_metric(self, value, aggregation=None, name=None)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        aggregation: Sample-wise metric reduction function. If `aggregation=None`,\n",
      " |          it indicates that the metric tensor provided has been aggregated\n",
      " |          already. eg, `bin_acc = BinaryAccuracy(name='acc')` followed by\n",
      " |          `model.add_metric(bin_acc(y_true, y_pred))`. If aggregation='mean', the\n",
      " |          given metric tensor will be sample-wise reduced using `mean` function.\n",
      " |          eg, `model.add_metric(tf.reduce_sum(outputs), name='output_mean',\n",
      " |          aggregation='mean')`.\n",
      " |        name: String metric name.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `aggregation` is anything other than None or `mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      The `get_updates_for` method allows to retrieve the updates relevant to a\n",
      " |      specific set of inputs.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Arguments:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: If anything other than None is passed, it signals the updates\n",
      " |          are conditional on some of the layer's inputs,\n",
      " |          and thus they should only be run where these inputs are available.\n",
      " |          This is the case for BatchNormalization updates, for instance.\n",
      " |          If None, the updates will be taken into account unconditionally,\n",
      " |          and you are responsible for making sure that any dependency they might\n",
      " |          have is available at runtime.\n",
      " |          A step counter might fall into this category.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Alias for `add_weight`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Apply the layer on a input.\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      Assumes that the layer will be built\n",
      " |      to match that input shape provided.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  dtype\n",
      " |  \n",
      " |  dynamic\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  losses\n",
      " |      Losses which are associated with this `Layer`.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |  \n",
      " |  name\n",
      " |      Returns the name of this module as passed or determined in the ctor.\n",
      " |      \n",
      " |      NOTE: This is not the same as the `self.name_scope.name` which includes\n",
      " |      parent module names.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of variables owned by this module and it's submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      ```\n",
      " |      class MyModule(tf.Module):\n",
      " |        @tf.Module.with_name_scope\n",
      " |        def __call__(self, x):\n",
      " |          if not hasattr(self, 'w'):\n",
      " |            self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))\n",
      " |          return tf.matmul(x, self.w)\n",
      " |      ```\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      ```\n",
      " |      mod = MyModule()\n",
      " |      mod(tf.ones([8, 32]))\n",
      " |      # ==> <tf.Tensor: ...>\n",
      " |      mod.w\n",
      " |      # ==> <tf.Variable ...'my_module/w:0'>\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      ```\n",
      " |      a = tf.Module()\n",
      " |      b = tf.Module()\n",
      " |      c = tf.Module()\n",
      " |      a.b = b\n",
      " |      b.c = c\n",
      " |      assert list(a.submodules) == [b, c]\n",
      " |      assert list(b.submodules) == [c]\n",
      " |      assert list(c.submodules) == []\n",
      " |      ```\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (500, 1, 20, 100) for Tensor 'Placeholder:0', which has shape '(?, 20, 100)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-214-9d5efd8bde3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             initial_state: test_state}\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mbatch_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorr_pdctn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdctns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msmall_predictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;31m#     print(pdctns.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#     print(pdctns.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1149\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1150\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (500, 1, 20, 100) for Tensor 'Placeholder:0', which has shape '(?, 20, 100)'"
     ]
    }
   ],
   "source": [
    "# Can I do this smaller sized?\n",
    "small_size = 1\n",
    "\n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"RNN_layers\"):\n",
    "        # Getting an initial state of all zeros\n",
    "        small_initial_state = cell.zero_state(small_size, tf.float32)\n",
    "        \n",
    "    small_outputs, small_final_state = tf.nn.dynamic_rnn(cell, inputs_,\n",
    "                                             initial_state=small_initial_state)\n",
    "    small_predictions = tf.contrib.layers.fully_connected(small_outputs[:, -1], \n",
    "                                                          output_size, activation_fn=tf.sigmoid)\n",
    "#     small_cost = tf.losses.mean_squared_error(labels_, small_predictions)\n",
    "#     small_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(small_cost)\n",
    "#     small_correct_pred = tf.equal(tf.cast(tf.round(small_predictions), tf.int32), labels_)\n",
    "#     small_accuracy = tf.reduce_mean(tf.cast(small_correct_pred, tf.float32))\n",
    "    \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, \"checkpoints/final_sentiment.ckpt\")\n",
    "    test_state = sess.run(cell.zero_state(small_size, tf.float32))\n",
    "    feed = {inputs_: np.array([test_word_vecs for x in range(500)]),\n",
    "            labels_: np.array([[0] * output_size]),\n",
    "            keep_prob: 1,\n",
    "            initial_state: test_state}\n",
    "    batch_acc, test_state, op, corr_pdctn, pdctns = sess.run([small_predictions], feed_dict=feed)\n",
    "#     print(pdctns.shape)\n",
    "#     print(pdctns.shape)\n",
    "#     print(pdctns[0])   \n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "#     #test_acc.append(batch_acc)\n",
    "#     #print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 20, 256)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def max_prediction(pdctns):\n",
    "    max_pred = max(pdctns[0])\n",
    "    return np.array([int(n == max_pred) for n in pdctns[0]])\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, \"checkpoints/final_sentiment.ckpt\")\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    #for ii, (bat_test_x, bat_test_y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "    for ii in range(1):\n",
    "        label_vec, inp_chunk = training_vects[ii]\n",
    "        full_inp_chunk = np.array([inp_chunk for n in range(500)])\n",
    "\n",
    "        feed = {inputs_: full_inp_chunk,\n",
    "                labels_: np.array([[1] * output_size]),\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        batch_acc, test_state, op, corr_pdctn, pdctns = sess.run([accuracy, final_state, outputs, correct_pred, predictions], feed_dict=feed)\n",
    "        print(op.shape)\n",
    "        machine_pred = max_prediction(pdctns)\n",
    "        print((label_vec * machine_pred).sum())\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 9])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "b = np.array([1,2,3])\n",
    "(a * b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.20197132),\n",
       " (1, 0.0024269521),\n",
       " (2, 0.011229187),\n",
       " (3, 0.5622808),\n",
       " (4, 0.0050757527),\n",
       " (5, 0.0041457117),\n",
       " (6, 0.024143666),\n",
       " (7, 0.011681408),\n",
       " (8, 0.0022202134),\n",
       " (9, 0.013743609),\n",
       " (10, 0.026958704),\n",
       " (11, 0.043389678),\n",
       " (12, 0.006714314),\n",
       " (13, 0.0013355911)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(pdctns[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.transform([label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['APPLICATION', 'BILL', 'BILL BINDER', 'BINDER',\n",
       "       'CANCELLATION NOTICE', 'CHANGE ENDORSEMENT', 'DECLARATION',\n",
       "       'DELETION OF INTEREST', 'EXPIRATION NOTICE',\n",
       "       'INTENT TO CANCEL NOTICE', 'NON-RENEWAL NOTICE', 'POLICY CHANGE',\n",
       "       'REINSTATEMENT NOTICE', 'RETURNED CHECK'], dtype=object)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias\n\t [[node rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias/read (defined at /var/folders/bc/6ycpk0y11zx4_kh4bljhdtf5dhvskt/T/tmpuf1_xpky.py:53) ]]\n\nOriginal stack trace for 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias/read':\n  File \"/Users/boaz.reisman/.pyenv/versions/3.6.5/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/boaz.reisman/.pyenv/versions/3.6.5/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/boaz.reisman/.pyenv/versions/3.6.5/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/Users/boaz.reisman/.pyenv/versions/3.6.5/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/Users/boaz.reisman/.pyenv/versions/3.6.5/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tornado/gen.py\", line 781, in inner\n    self.run()\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tornado/gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-17-94a791623b59>\", line 3, in <module>\n    initial_state=initial_state)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\n    return func(*args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 707, in dynamic_rnn\n    dtype=dtype)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 916, in _dynamic_rnn_loop\n    swap_memory=swap_memory)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3501, in while_loop\n    return_same_structure)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3012, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2937, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3456, in <lambda>\n    body = lambda i, lv: (i + 1, orig_body(*lv))\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 884, in _time_step\n    (output, new_state) = call_cell()\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 870, in <lambda>\n    call_cell = lambda: cell(input_t, state)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 248, in __call__\n    return super(RNNCell, self).__call__(inputs, state)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 537, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 634, in __call__\n    outputs = call_fn(inputs, *args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 146, in wrapper\n    ), args, kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 450, in converted_call\n    result = converted_f(*effective_args, **kwargs)\n  File \"/var/folders/bc/6ycpk0y11zx4_kh4bljhdtf5dhvskt/T/tmpuf1_xpky.py\", line 56, in tf__call\n    cur_state_pos, cur_inp = ag__.for_stmt(ag__.converted_call(enumerate, None, ag__.ConversionOptions(recursive=True, force_conversion=False, optional_features=(), internal_convert_user_code=True), (self._cells,), None), None, loop_body, (cur_state_pos, cur_inp))\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 110, in for_stmt\n    return _py_for_stmt(iter_, extra_test, body, init_state)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 119, in _py_for_stmt\n    state = body(target, *state)\n  File \"/var/folders/bc/6ycpk0y11zx4_kh4bljhdtf5dhvskt/T/tmpuf1_xpky.py\", line 53, in loop_body\n    cur_inp_1, new_state = ag__.converted_call(cell, None, ag__.ConversionOptions(recursive=True, force_conversion=False, optional_features=(), internal_convert_user_code=True), (cur_inp_1, cur_state), None)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 356, in converted_call\n    return _call_unconverted(f, args, kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 255, in _call_unconverted\n    return f(*args)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1159, in __call__\n    inputs, state, cell_call_fn=self.cell.__call__, scope=scope)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1436, in _call_wrapped_cell\n    output, new_state = cell_call_fn(inputs, state, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 385, in __call__\n    self, inputs, state, scope=scope, *args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 537, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 591, in __call__\n    self._maybe_build(inputs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1881, in _maybe_build\n    self.build(input_shapes)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py\", line 295, in wrapper\n    output_shape = fn(instance, input_shape)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 738, in build\n    initializer=init_ops.zeros_initializer(dtype=self.dtype))\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1484, in add_variable\n    return self.add_weight(*args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 450, in add_weight\n    **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 384, in add_weight\n    aggregation=aggregation)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\", line 663, in _add_variable_with_custom_getter\n    **kwargs_for_getter)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1496, in get_variable\n    aggregation=aggregation)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1239, in get_variable\n    aggregation=aggregation)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 545, in get_variable\n    return custom_getter(**custom_getter_kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 251, in _rnn_get_variable\n    variable = getter(*args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 514, in _true_getter\n    aggregation=aggregation)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 929, in _get_single_variable\n    aggregation=aggregation)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 259, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 220, in _variable_v1_call\n    shape=shape)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 198, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2511, in default_variable_creator\n    shape=shape)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 263, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 1568, in __init__\n    shape=shape)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 1752, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 86, in identity\n    ret = gen_array_ops.identity(input, name=name)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4253, in identity\n    \"Identity\", input=input, name=name)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias\n\t [[{{node rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias/read}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-dcf2fe9680ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcorrect_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1368\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias\n\t [[node rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias/read (defined at /var/folders/bc/6ycpk0y11zx4_kh4bljhdtf5dhvskt/T/tmpuf1_xpky.py:53) ]]\n\nOriginal stack trace for 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias/read':\n  File \"/Users/boaz.reisman/.pyenv/versions/3.6.5/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/boaz.reisman/.pyenv/versions/3.6.5/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/boaz.reisman/.pyenv/versions/3.6.5/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/Users/boaz.reisman/.pyenv/versions/3.6.5/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/Users/boaz.reisman/.pyenv/versions/3.6.5/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tornado/gen.py\", line 781, in inner\n    self.run()\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tornado/gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-17-94a791623b59>\", line 3, in <module>\n    initial_state=initial_state)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\n    return func(*args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 707, in dynamic_rnn\n    dtype=dtype)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 916, in _dynamic_rnn_loop\n    swap_memory=swap_memory)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3501, in while_loop\n    return_same_structure)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3012, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2937, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3456, in <lambda>\n    body = lambda i, lv: (i + 1, orig_body(*lv))\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 884, in _time_step\n    (output, new_state) = call_cell()\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 870, in <lambda>\n    call_cell = lambda: cell(input_t, state)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 248, in __call__\n    return super(RNNCell, self).__call__(inputs, state)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 537, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 634, in __call__\n    outputs = call_fn(inputs, *args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 146, in wrapper\n    ), args, kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 450, in converted_call\n    result = converted_f(*effective_args, **kwargs)\n  File \"/var/folders/bc/6ycpk0y11zx4_kh4bljhdtf5dhvskt/T/tmpuf1_xpky.py\", line 56, in tf__call\n    cur_state_pos, cur_inp = ag__.for_stmt(ag__.converted_call(enumerate, None, ag__.ConversionOptions(recursive=True, force_conversion=False, optional_features=(), internal_convert_user_code=True), (self._cells,), None), None, loop_body, (cur_state_pos, cur_inp))\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 110, in for_stmt\n    return _py_for_stmt(iter_, extra_test, body, init_state)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/autograph/operators/control_flow.py\", line 119, in _py_for_stmt\n    state = body(target, *state)\n  File \"/var/folders/bc/6ycpk0y11zx4_kh4bljhdtf5dhvskt/T/tmpuf1_xpky.py\", line 53, in loop_body\n    cur_inp_1, new_state = ag__.converted_call(cell, None, ag__.ConversionOptions(recursive=True, force_conversion=False, optional_features=(), internal_convert_user_code=True), (cur_inp_1, cur_state), None)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 356, in converted_call\n    return _call_unconverted(f, args, kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 255, in _call_unconverted\n    return f(*args)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1159, in __call__\n    inputs, state, cell_call_fn=self.cell.__call__, scope=scope)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1436, in _call_wrapped_cell\n    output, new_state = cell_call_fn(inputs, state, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 385, in __call__\n    self, inputs, state, scope=scope, *args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 537, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 591, in __call__\n    self._maybe_build(inputs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1881, in _maybe_build\n    self.build(input_shapes)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py\", line 295, in wrapper\n    output_shape = fn(instance, input_shape)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 738, in build\n    initializer=init_ops.zeros_initializer(dtype=self.dtype))\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1484, in add_variable\n    return self.add_weight(*args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 450, in add_weight\n    **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 384, in add_weight\n    aggregation=aggregation)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\", line 663, in _add_variable_with_custom_getter\n    **kwargs_for_getter)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1496, in get_variable\n    aggregation=aggregation)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1239, in get_variable\n    aggregation=aggregation)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 545, in get_variable\n    return custom_getter(**custom_getter_kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 251, in _rnn_get_variable\n    variable = getter(*args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 514, in _true_getter\n    aggregation=aggregation)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 929, in _get_single_variable\n    aggregation=aggregation)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 259, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 220, in _variable_v1_call\n    shape=shape)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 198, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2511, in default_variable_creator\n    shape=shape)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 263, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 1568, in __init__\n    shape=shape)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 1752, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 86, in identity\n    ret = gen_array_ops.identity(input, name=name)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4253, in identity\n    \"Identity\", input=input, name=name)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/Users/boaz.reisman/.virtualenvs/datascience/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.Print(correct_pred, [correct_pred])\n",
    "    sess.run(correct_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Tensor in module tensorflow.python.framework.ops object:\n",
      "\n",
      "class Tensor(_TensorLike)\n",
      " |  Represents one of the outputs of an `Operation`.\n",
      " |  \n",
      " |  A `Tensor` is a symbolic handle to one of the outputs of an\n",
      " |  `Operation`. It does not hold the values of that operation's output,\n",
      " |  but instead provides a means of computing those values in a\n",
      " |  TensorFlow `tf.compat.v1.Session`.\n",
      " |  \n",
      " |  This class has two primary purposes:\n",
      " |  \n",
      " |  1. A `Tensor` can be passed as an input to another `Operation`.\n",
      " |     This builds a dataflow connection between operations, which\n",
      " |     enables TensorFlow to execute an entire `Graph` that represents a\n",
      " |     large, multi-step computation.\n",
      " |  \n",
      " |  2. After the graph has been launched in a session, the value of the\n",
      " |     `Tensor` can be computed by passing it to\n",
      " |     `tf.Session.run`.\n",
      " |     `t.eval()` is a shortcut for calling\n",
      " |     `tf.compat.v1.get_default_session().run(t)`.\n",
      " |  \n",
      " |  In the following example, `c`, `d`, and `e` are symbolic `Tensor`\n",
      " |  objects, whereas `result` is a numpy array that stores a concrete\n",
      " |  value:\n",
      " |  \n",
      " |  ```python\n",
      " |  # Build a dataflow graph.\n",
      " |  c = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
      " |  d = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n",
      " |  e = tf.matmul(c, d)\n",
      " |  \n",
      " |  # Construct a `Session` to execute the graph.\n",
      " |  sess = tf.compat.v1.Session()\n",
      " |  \n",
      " |  # Execute the graph and store the value that `e` represents in `result`.\n",
      " |  result = sess.run(e)\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Tensor\n",
      " |      _TensorLike\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __abs__ = abs(x, name=None)\n",
      " |      Computes the absolute value of a tensor.\n",
      " |      \n",
      " |      Given a tensor of integer or floating-point values, this operation returns a\n",
      " |      tensor of the same type, where each element contains the absolute value of the\n",
      " |      corresponding element in the input.\n",
      " |      \n",
      " |      Given a tensor `x` of complex numbers, this operation returns a tensor of type\n",
      " |      `float32` or `float64` that is the absolute value of each element in `x`. All\n",
      " |      elements in `x` must be complex numbers of the form \\\\(a + bj\\\\). The\n",
      " |      absolute value is computed as \\\\( \\sqrt{a^2 + b^2}\\\\).  For example:\n",
      " |      ```python\n",
      " |      x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])\n",
      " |      tf.abs(x)  # [5.25594902, 6.60492229]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` or `SparseTensor` of type `float16`, `float32`, `float64`,\n",
      " |          `int32`, `int64`, `complex64` or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` or `SparseTensor` the same size, type, and sparsity as `x` with\n",
      " |          absolute values.\n",
      " |        Note, for `complex64` or `complex128` input, the returned `Tensor` will be\n",
      " |          of type `float32` or `float64`, respectively.\n",
      " |      \n",
      " |        If `x` is a `SparseTensor`, returns\n",
      " |        `SparseTensor(x.indices, tf.math.abs(x.values, ...), x.dense_shape)`\n",
      " |  \n",
      " |  __add__ = binary_op_wrapper(x, y)\n",
      " |      Returns x + y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.add` supports broadcasting. `AddN` does not. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __and__ = binary_op_wrapper(x, y)\n",
      " |      Returns the truth value of x AND y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_and` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __bool__(self)\n",
      " |      Dummy method to prevent a tensor from being used as a Python `bool`.\n",
      " |      \n",
      " |      This overload raises a `TypeError` when the user inadvertently\n",
      " |      treats a `Tensor` as a boolean (e.g. in an `if` statement). For\n",
      " |      example:\n",
      " |      \n",
      " |      ```python\n",
      " |      if tf.constant(True):  # Will raise.\n",
      " |        # ...\n",
      " |      \n",
      " |      if tf.constant(5) < tf.constant(7):  # Will raise.\n",
      " |        # ...\n",
      " |      ```\n",
      " |      \n",
      " |      This disallows ambiguities between testing the Python value vs testing the\n",
      " |      dynamic condition of the `Tensor`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        `TypeError`.\n",
      " |  \n",
      " |  __copy__(self)\n",
      " |  \n",
      " |  __div__ = binary_op_wrapper(x, y)\n",
      " |      Divide two values using Python 2 semantics.\n",
      " |      \n",
      " |      Used for Tensor.__div__.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` returns the quotient of x and y.\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __floordiv__ = binary_op_wrapper(x, y)\n",
      " |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
      " |      \n",
      " |      The same as `tf.compat.v1.div(x,y)` for integers, but uses\n",
      " |      `tf.floor(tf.compat.v1.div(x,y))` for\n",
      " |      floating point arguments so that the result is always an integer (though\n",
      " |      possibly an integer represented as floating point).  This op is generated by\n",
      " |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.\n",
      " |      \n",
      " |      `x` and `y` must have the same type, and the result will have the same type\n",
      " |      as well.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` rounded down.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the inputs are complex.\n",
      " |  \n",
      " |  __ge__ = greater_equal(x, y, name=None)\n",
      " |      Returns the truth value of (x >= y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.greater_equal` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __getitem__ = _slice_helper(tensor, slice_spec, var=None)\n",
      " |      Overload for Tensor.__getitem__.\n",
      " |      \n",
      " |      This operation extracts the specified region from the tensor.\n",
      " |      The notation is similar to NumPy with the restriction that\n",
      " |      currently only support basic indexing. That means that\n",
      " |      using a non-scalar tensor as input is not currently allowed.\n",
      " |      \n",
      " |      Some useful examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Strip leading and trailing 2 elements\n",
      " |      foo = tf.constant([1,2,3,4,5,6])\n",
      " |      print(foo[2:-2].eval())  # => [3,4]\n",
      " |      \n",
      " |      # Skip every other row and reverse the order of the columns\n",
      " |      foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\n",
      " |      print(foo[::2,::-1].eval())  # => [[3,2,1], [9,8,7]]\n",
      " |      \n",
      " |      # Use scalar tensors as indices on both dimensions\n",
      " |      print(foo[tf.constant(0), tf.constant(2)].eval())  # => 3\n",
      " |      \n",
      " |      # Insert another dimension\n",
      " |      foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\n",
      " |      print(foo[tf.newaxis, :, :].eval()) # => [[[1,2,3], [4,5,6], [7,8,9]]]\n",
      " |      print(foo[:, tf.newaxis, :].eval()) # => [[[1,2,3]], [[4,5,6]], [[7,8,9]]]\n",
      " |      print(foo[:, :, tf.newaxis].eval()) # => [[[1],[2],[3]], [[4],[5],[6]],\n",
      " |      [[7],[8],[9]]]\n",
      " |      \n",
      " |      # Ellipses (3 equivalent operations)\n",
      " |      foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\n",
      " |      print(foo[tf.newaxis, :, :].eval())  # => [[[1,2,3], [4,5,6], [7,8,9]]]\n",
      " |      print(foo[tf.newaxis, ...].eval())  # => [[[1,2,3], [4,5,6], [7,8,9]]]\n",
      " |      print(foo[tf.newaxis].eval())  # => [[[1,2,3], [4,5,6], [7,8,9]]]\n",
      " |      \n",
      " |      # Masks\n",
      " |      foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\n",
      " |      print(foo[foo > 2].eval())  # => [3, 4, 5, 6, 7, 8, 9]\n",
      " |      ```\n",
      " |      \n",
      " |      Notes:\n",
      " |        - `tf.newaxis` is `None` as in NumPy.\n",
      " |        - An implicit ellipsis is placed at the end of the `slice_spec`\n",
      " |        - NumPy advanced indexing is currently not supported.\n",
      " |      \n",
      " |      Args:\n",
      " |        tensor: An ops.Tensor object.\n",
      " |        slice_spec: The arguments to Tensor.__getitem__.\n",
      " |        var: In the case of variable slice assignment, the Variable object to slice\n",
      " |          (i.e. tensor is the read-only view of this variable).\n",
      " |      \n",
      " |      Returns:\n",
      " |        The appropriate slice of \"tensor\", based on \"slice_spec\".\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If a slice range is negative size.\n",
      " |        TypeError: If the slice indices aren't int, slice, ellipsis,\n",
      " |          tf.newaxis or scalar int32/int64 tensors.\n",
      " |  \n",
      " |  __gt__ = greater(x, y, name=None)\n",
      " |      Returns the truth value of (x > y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.greater` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __init__(self, op, value_index, dtype)\n",
      " |      Creates a new `Tensor`.\n",
      " |      \n",
      " |      Args:\n",
      " |        op: An `Operation`. `Operation` that computes this tensor.\n",
      " |        value_index: An `int`. Index of the operation's endpoint that produces\n",
      " |          this tensor.\n",
      " |        dtype: A `DType`. Type of elements stored in this tensor.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the op is not an `Operation`.\n",
      " |  \n",
      " |  __invert__ = logical_not(x, name=None)\n",
      " |      Returns the truth value of NOT x element-wise.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __le__ = less_equal(x, y, name=None)\n",
      " |      Returns the truth value of (x <= y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.less_equal` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __lt__ = less(x, y, name=None)\n",
      " |      Returns the truth value of (x < y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.less` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __matmul__ = binary_op_wrapper(x, y)\n",
      " |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
      " |      \n",
      " |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
      " |      where the inner 2 dimensions specify valid matrix multiplication arguments,\n",
      " |      and any further outer dimensions match.\n",
      " |      \n",
      " |      Both matrices must be of the same type. The supported types are:\n",
      " |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
      " |      \n",
      " |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
      " |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
      " |      by default.\n",
      " |      \n",
      " |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
      " |      multiplication algorithm can be used by setting the corresponding\n",
      " |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
      " |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
      " |      datatypes `bfloat16` or `float32`.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # 2-D tensor `a`\n",
      " |      # [[1, 2, 3],\n",
      " |      #  [4, 5, 6]]\n",
      " |      a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
      " |      \n",
      " |      # 2-D tensor `b`\n",
      " |      # [[ 7,  8],\n",
      " |      #  [ 9, 10],\n",
      " |      #  [11, 12]]\n",
      " |      b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[ 58,  64],\n",
      " |      #  [139, 154]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      \n",
      " |      # 3-D tensor `a`\n",
      " |      # [[[ 1,  2,  3],\n",
      " |      #   [ 4,  5,  6]],\n",
      " |      #  [[ 7,  8,  9],\n",
      " |      #   [10, 11, 12]]]\n",
      " |      a = tf.constant(np.arange(1, 13, dtype=np.int32),\n",
      " |                      shape=[2, 2, 3])\n",
      " |      \n",
      " |      # 3-D tensor `b`\n",
      " |      # [[[13, 14],\n",
      " |      #   [15, 16],\n",
      " |      #   [17, 18]],\n",
      " |      #  [[19, 20],\n",
      " |      #   [21, 22],\n",
      " |      #   [23, 24]]]\n",
      " |      b = tf.constant(np.arange(13, 25, dtype=np.int32),\n",
      " |                      shape=[2, 3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[[ 94, 100],\n",
      " |      #   [229, 244]],\n",
      " |      #  [[508, 532],\n",
      " |      #   [697, 730]]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      # Since python >= 3.5 the @ operator is supported (see PEP 465).\n",
      " |      # In TensorFlow, it simply calls the `tf.matmul()` function, so the\n",
      " |      # following lines are equivalent:\n",
      " |      d = a @ b @ [[10.], [11.]]\n",
      " |      d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        a: `Tensor` of type `float16`, `float32`, `float64`, `int32`, `complex64`,\n",
      " |          `complex128` and rank > 1.\n",
      " |        b: `Tensor` with same type and rank as `a`.\n",
      " |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
      " |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
      " |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        a_is_sparse: If `True`, `a` is treated as a sparse matrix.\n",
      " |        b_is_sparse: If `True`, `b` is treated as a sparse matrix.\n",
      " |        name: Name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of the same type as `a` and `b` where each inner-most matrix is\n",
      " |        the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
      " |        transpose or adjoint attributes are `False`:\n",
      " |      \n",
      " |        `output`[..., i, j] = sum_k (`a`[..., i, k] * `b`[..., k, j]),\n",
      " |        for all indices i, j.\n",
      " |      \n",
      " |        Note: This is matrix product, not element-wise product.\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If transpose_a and adjoint_a, or transpose_b and adjoint_b\n",
      " |          are both set to True.\n",
      " |  \n",
      " |  __mod__ = binary_op_wrapper(x, y)\n",
      " |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
      " |      \n",
      " |      true, this follows Python semantics in that the result here is consistent\n",
      " |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
      " |      \n",
      " |      *NOTE*: `math.floormod` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __mul__ = binary_op_wrapper(x, y)\n",
      " |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
      " |  \n",
      " |  __neg__ = neg(x, name=None)\n",
      " |      Computes numerical negative value element-wise.\n",
      " |      \n",
      " |      I.e., \\\\(y = -x\\\\).\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |      \n",
      " |        If `x` is a `SparseTensor`, returns\n",
      " |        `SparseTensor(x.indices, tf.math.negative(x.values, ...), x.dense_shape)`\n",
      " |  \n",
      " |  __nonzero__(self)\n",
      " |      Dummy method to prevent a tensor from being used as a Python `bool`.\n",
      " |      \n",
      " |      This is the Python 2.x counterpart to `__bool__()` above.\n",
      " |      \n",
      " |      Raises:\n",
      " |        `TypeError`.\n",
      " |  \n",
      " |  __or__ = binary_op_wrapper(x, y)\n",
      " |      Returns the truth value of x OR y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_or` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __pow__ = binary_op_wrapper(x, y)\n",
      " |      Computes the power of one value to another.\n",
      " |      \n",
      " |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
      " |      corresponding elements in `x` and `y`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([[2, 2], [3, 3]])\n",
      " |      y = tf.constant([[8, 16], [2, 3]])\n",
      " |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |          `complex64`, or `complex128`.\n",
      " |        y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |          `complex64`, or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  __radd__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns x + y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.add` supports broadcasting. `AddN` does not. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rand__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns the truth value of x AND y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_and` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __rdiv__ = r_binary_op_wrapper(y, x)\n",
      " |      Divide two values using Python 2 semantics.\n",
      " |      \n",
      " |      Used for Tensor.__div__.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` returns the quotient of x and y.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rfloordiv__ = r_binary_op_wrapper(y, x)\n",
      " |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
      " |      \n",
      " |      The same as `tf.compat.v1.div(x,y)` for integers, but uses\n",
      " |      `tf.floor(tf.compat.v1.div(x,y))` for\n",
      " |      floating point arguments so that the result is always an integer (though\n",
      " |      possibly an integer represented as floating point).  This op is generated by\n",
      " |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.\n",
      " |      \n",
      " |      `x` and `y` must have the same type, and the result will have the same type\n",
      " |      as well.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` rounded down.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the inputs are complex.\n",
      " |  \n",
      " |  __rmatmul__ = r_binary_op_wrapper(y, x)\n",
      " |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
      " |      \n",
      " |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
      " |      where the inner 2 dimensions specify valid matrix multiplication arguments,\n",
      " |      and any further outer dimensions match.\n",
      " |      \n",
      " |      Both matrices must be of the same type. The supported types are:\n",
      " |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
      " |      \n",
      " |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
      " |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
      " |      by default.\n",
      " |      \n",
      " |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
      " |      multiplication algorithm can be used by setting the corresponding\n",
      " |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
      " |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
      " |      datatypes `bfloat16` or `float32`.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # 2-D tensor `a`\n",
      " |      # [[1, 2, 3],\n",
      " |      #  [4, 5, 6]]\n",
      " |      a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
      " |      \n",
      " |      # 2-D tensor `b`\n",
      " |      # [[ 7,  8],\n",
      " |      #  [ 9, 10],\n",
      " |      #  [11, 12]]\n",
      " |      b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[ 58,  64],\n",
      " |      #  [139, 154]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      \n",
      " |      # 3-D tensor `a`\n",
      " |      # [[[ 1,  2,  3],\n",
      " |      #   [ 4,  5,  6]],\n",
      " |      #  [[ 7,  8,  9],\n",
      " |      #   [10, 11, 12]]]\n",
      " |      a = tf.constant(np.arange(1, 13, dtype=np.int32),\n",
      " |                      shape=[2, 2, 3])\n",
      " |      \n",
      " |      # 3-D tensor `b`\n",
      " |      # [[[13, 14],\n",
      " |      #   [15, 16],\n",
      " |      #   [17, 18]],\n",
      " |      #  [[19, 20],\n",
      " |      #   [21, 22],\n",
      " |      #   [23, 24]]]\n",
      " |      b = tf.constant(np.arange(13, 25, dtype=np.int32),\n",
      " |                      shape=[2, 3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[[ 94, 100],\n",
      " |      #   [229, 244]],\n",
      " |      #  [[508, 532],\n",
      " |      #   [697, 730]]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      # Since python >= 3.5 the @ operator is supported (see PEP 465).\n",
      " |      # In TensorFlow, it simply calls the `tf.matmul()` function, so the\n",
      " |      # following lines are equivalent:\n",
      " |      d = a @ b @ [[10.], [11.]]\n",
      " |      d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        a: `Tensor` of type `float16`, `float32`, `float64`, `int32`, `complex64`,\n",
      " |          `complex128` and rank > 1.\n",
      " |        b: `Tensor` with same type and rank as `a`.\n",
      " |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
      " |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
      " |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        a_is_sparse: If `True`, `a` is treated as a sparse matrix.\n",
      " |        b_is_sparse: If `True`, `b` is treated as a sparse matrix.\n",
      " |        name: Name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of the same type as `a` and `b` where each inner-most matrix is\n",
      " |        the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
      " |        transpose or adjoint attributes are `False`:\n",
      " |      \n",
      " |        `output`[..., i, j] = sum_k (`a`[..., i, k] * `b`[..., k, j]),\n",
      " |        for all indices i, j.\n",
      " |      \n",
      " |        Note: This is matrix product, not element-wise product.\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If transpose_a and adjoint_a, or transpose_b and adjoint_b\n",
      " |          are both set to True.\n",
      " |  \n",
      " |  __rmod__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
      " |      \n",
      " |      true, this follows Python semantics in that the result here is consistent\n",
      " |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
      " |      \n",
      " |      *NOTE*: `math.floormod` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rmul__ = r_binary_op_wrapper(y, x)\n",
      " |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
      " |  \n",
      " |  __ror__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns the truth value of x OR y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_or` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __rpow__ = r_binary_op_wrapper(y, x)\n",
      " |      Computes the power of one value to another.\n",
      " |      \n",
      " |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
      " |      corresponding elements in `x` and `y`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([[2, 2], [3, 3]])\n",
      " |      y = tf.constant([[8, 16], [2, 3]])\n",
      " |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |          `complex64`, or `complex128`.\n",
      " |        y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |          `complex64`, or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  __rsub__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns x - y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rtruediv__ = r_binary_op_wrapper(y, x)\n",
      " |  \n",
      " |  __rxor__ = r_binary_op_wrapper(y, x)\n",
      " |      Logical XOR function.\n",
      " |      \n",
      " |      x ^ y = (x | y) & ~(x & y)\n",
      " |      \n",
      " |      Inputs are tensor and if the tensors contains more than one element, an\n",
      " |      element-wise logical XOR is computed.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([False, False, True, True], dtype = tf.bool)\n",
      " |      y = tf.constant([False, True, False, True], dtype = tf.bool)\n",
      " |      z = tf.logical_xor(x, y, name=\"LogicalXor\")\n",
      " |      #  here z = [False  True  True False]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          x: A `Tensor` type bool.\n",
      " |          y: A `Tensor` of type bool.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type bool with the same size as that of x or y.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  __sub__ = binary_op_wrapper(x, y)\n",
      " |      Returns x - y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __truediv__ = binary_op_wrapper(x, y)\n",
      " |  \n",
      " |  __xor__ = binary_op_wrapper(x, y)\n",
      " |      Logical XOR function.\n",
      " |      \n",
      " |      x ^ y = (x | y) & ~(x & y)\n",
      " |      \n",
      " |      Inputs are tensor and if the tensors contains more than one element, an\n",
      " |      element-wise logical XOR is computed.\n",
      " |      \n",
      " |      Usage:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([False, False, True, True], dtype = tf.bool)\n",
      " |      y = tf.constant([False, True, False, True], dtype = tf.bool)\n",
      " |      z = tf.logical_xor(x, y, name=\"LogicalXor\")\n",
      " |      #  here z = [False  True  True False]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          x: A `Tensor` type bool.\n",
      " |          y: A `Tensor` of type bool.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type bool with the same size as that of x or y.\n",
      " |  \n",
      " |  consumers(self)\n",
      " |      Returns a list of `Operation`s that consume this tensor.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of `Operation`s.\n",
      " |  \n",
      " |  eval(self, feed_dict=None, session=None)\n",
      " |      Evaluates this tensor in a `Session`.\n",
      " |      \n",
      " |      Calling this method will execute all preceding operations that\n",
      " |      produce the inputs needed for the operation that produces this\n",
      " |      tensor.\n",
      " |      \n",
      " |      *N.B.* Before invoking `Tensor.eval()`, its graph must have been\n",
      " |      launched in a session, and either a default session must be\n",
      " |      available, or `session` must be specified explicitly.\n",
      " |      \n",
      " |      Args:\n",
      " |        feed_dict: A dictionary that maps `Tensor` objects to feed values. See\n",
      " |          `tf.Session.run` for a description of the valid feed values.\n",
      " |        session: (Optional.) The `Session` to be used to evaluate this tensor. If\n",
      " |          none, the default session will be used.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A numpy array corresponding to the value of this tensor.\n",
      " |  \n",
      " |  get_shape(self)\n",
      " |      Alias of Tensor.shape.\n",
      " |  \n",
      " |  set_shape(self, shape)\n",
      " |      Updates the shape of this tensor.\n",
      " |      \n",
      " |      This method can be called multiple times, and will merge the given\n",
      " |      `shape` with the current shape of this tensor. It can be used to\n",
      " |      provide additional information about the shape of this tensor that\n",
      " |      cannot be inferred from the graph alone. For example, this can be used\n",
      " |      to provide additional information about the shapes of images:\n",
      " |      \n",
      " |      ```python\n",
      " |      _, image_data = tf.compat.v1.TFRecordReader(...).read(...)\n",
      " |      image = tf.image.decode_png(image_data, channels=3)\n",
      " |      \n",
      " |      # The height and width dimensions of `image` are data dependent, and\n",
      " |      # cannot be computed without executing the op.\n",
      " |      print(image.shape)\n",
      " |      ==> TensorShape([Dimension(None), Dimension(None), Dimension(3)])\n",
      " |      \n",
      " |      # We know that each image in this dataset is 28 x 28 pixels.\n",
      " |      image.set_shape([28, 28, 3])\n",
      " |      print(image.shape)\n",
      " |      ==> TensorShape([Dimension(28), Dimension(28), Dimension(3)])\n",
      " |      ```\n",
      " |      \n",
      " |      NOTE: This shape is not enforced at runtime. Setting incorrect shapes can\n",
      " |      result in inconsistencies between the statically-known graph and the runtime\n",
      " |      value of tensors. For runtime validation of the shape, use `tf.ensure_shape`\n",
      " |      instead.\n",
      " |      \n",
      " |      Args:\n",
      " |        shape: A `TensorShape` representing the shape of this tensor, a\n",
      " |          `TensorShapeProto`, a list, a tuple, or None.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `shape` is not compatible with the current shape of\n",
      " |          this tensor.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  device\n",
      " |      The name of the device on which this tensor will be produced, or None.\n",
      " |  \n",
      " |  dtype\n",
      " |      The `DType` of elements in this tensor.\n",
      " |  \n",
      " |  graph\n",
      " |      The `Graph` that contains this tensor.\n",
      " |  \n",
      " |  name\n",
      " |      The string name of this tensor.\n",
      " |  \n",
      " |  op\n",
      " |      The `Operation` that produces this tensor as an output.\n",
      " |  \n",
      " |  shape\n",
      " |      Returns the `TensorShape` that represents the shape of this tensor.\n",
      " |      \n",
      " |      The shape is computed using shape inference functions that are\n",
      " |      registered in the Op for each `Operation`.  See\n",
      " |      `tf.TensorShape`\n",
      " |      for more details of what a shape represents.\n",
      " |      \n",
      " |      The inferred shape of a tensor is used to provide shape\n",
      " |      information without having to launch the graph in a session. This\n",
      " |      can be used for debugging, and providing early error messages. For\n",
      " |      example:\n",
      " |      \n",
      " |      ```python\n",
      " |      c = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
      " |      \n",
      " |      print(c.shape)\n",
      " |      ==> TensorShape([Dimension(2), Dimension(3)])\n",
      " |      \n",
      " |      d = tf.constant([[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]])\n",
      " |      \n",
      " |      print(d.shape)\n",
      " |      ==> TensorShape([Dimension(4), Dimension(2)])\n",
      " |      \n",
      " |      # Raises a ValueError, because `c` and `d` do not have compatible\n",
      " |      # inner dimensions.\n",
      " |      e = tf.matmul(c, d)\n",
      " |      \n",
      " |      f = tf.matmul(c, d, transpose_a=True, transpose_b=True)\n",
      " |      \n",
      " |      print(f.shape)\n",
      " |      ==> TensorShape([Dimension(3), Dimension(4)])\n",
      " |      ```\n",
      " |      \n",
      " |      In some cases, the inferred shape may have unknown dimensions. If\n",
      " |      the caller has additional information about the values of these\n",
      " |      dimensions, `Tensor.set_shape()` can be used to augment the\n",
      " |      inferred shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `TensorShape` representing the shape of this tensor.\n",
      " |  \n",
      " |  value_index\n",
      " |      The index of this tensor in the outputs of its `Operation`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  OVERLOADABLE_OPERATORS = {'__abs__', '__add__', '__and__', '__div__', ...\n",
      " |  \n",
      " |  __array_priority__ = 100\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _TensorLike:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(correct_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'PrintV2_1' type=PrintV2>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.print(correct_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "run() missing 2 required positional arguments: 'self' and 'fetches'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-1de345b1dda0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: run() missing 2 required positional arguments: 'self' and 'fetches'"
     ]
    }
   ],
   "source": [
    "tf.Session.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

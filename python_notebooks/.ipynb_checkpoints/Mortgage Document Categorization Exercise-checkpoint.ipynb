{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook, I explore the data given and discover a technique for accurately predicting the document classifiation for the anonymized text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we import the CSV and take an initial look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DELETION OF INTEREST</td>\n",
       "      <td>e04a09c87692 d6b72e591b91 5d066f0246f1 ed41171...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RETURNED CHECK</td>\n",
       "      <td>a3b334c6eefd be95012ebf2b 41d67080e078 ff1c26e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BILL</td>\n",
       "      <td>586242498a88 9ccf259ca087 54709b24b45f 6bf9c0c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BILL</td>\n",
       "      <td>cd50e861f48b 6ca2dd348663 d38820625542 f077614...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BILL</td>\n",
       "      <td>9db5536263d8 1c303d15eb65 3f89b4673455 b73e657...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0                                                  1\n",
       "0  DELETION OF INTEREST  e04a09c87692 d6b72e591b91 5d066f0246f1 ed41171...\n",
       "1        RETURNED CHECK  a3b334c6eefd be95012ebf2b 41d67080e078 ff1c26e...\n",
       "2                  BILL  586242498a88 9ccf259ca087 54709b24b45f 6bf9c0c...\n",
       "3                  BILL  cd50e861f48b 6ca2dd348663 d38820625542 f077614...\n",
       "4                  BILL  9db5536263d8 1c303d15eb65 3f89b4673455 b73e657..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"../shuffled-full-set-hashed.csv\", header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows 62204\n",
      "\n",
      "Most Common Categories:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('BILL', 18968),\n",
       " ('POLICY CHANGE', 10627),\n",
       " ('CANCELLATION NOTICE', 9731),\n",
       " ('BINDER', 8973),\n",
       " ('DELETION OF INTEREST', 4826),\n",
       " ('REINSTATEMENT NOTICE', 4368),\n",
       " ('DECLARATION', 968),\n",
       " ('CHANGE ENDORSEMENT', 889),\n",
       " ('RETURNED CHECK', 749),\n",
       " ('EXPIRATION NOTICE', 734),\n",
       " ('NON-RENEWAL NOTICE', 624),\n",
       " ('BILL BINDER', 289),\n",
       " ('INTENT TO CANCEL NOTICE', 229),\n",
       " ('APPLICATION', 229)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Rows %d\"%len(df[0]))\n",
    "\n",
    "print(\"\\nMost Common Categories:\")\n",
    "cntr = collections.Counter(df[0])\n",
    "cntr.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not a very even distribution of categories, what about the distribution of word counts within each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DELETION OF INTEREST</td>\n",
       "      <td>e04a09c87692 d6b72e591b91 5d066f0246f1 ed41171...</td>\n",
       "      <td>465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RETURNED CHECK</td>\n",
       "      <td>a3b334c6eefd be95012ebf2b 41d67080e078 ff1c26e...</td>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BILL</td>\n",
       "      <td>586242498a88 9ccf259ca087 54709b24b45f 6bf9c0c...</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BILL</td>\n",
       "      <td>cd50e861f48b 6ca2dd348663 d38820625542 f077614...</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BILL</td>\n",
       "      <td>9db5536263d8 1c303d15eb65 3f89b4673455 b73e657...</td>\n",
       "      <td>546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0                                                  1  \\\n",
       "0  DELETION OF INTEREST  e04a09c87692 d6b72e591b91 5d066f0246f1 ed41171...   \n",
       "1        RETURNED CHECK  a3b334c6eefd be95012ebf2b 41d67080e078 ff1c26e...   \n",
       "2                  BILL  586242498a88 9ccf259ca087 54709b24b45f 6bf9c0c...   \n",
       "3                  BILL  cd50e861f48b 6ca2dd348663 d38820625542 f077614...   \n",
       "4                  BILL  9db5536263d8 1c303d15eb65 3f89b4673455 b73e657...   \n",
       "\n",
       "   word_count  \n",
       "0         465  \n",
       "1         403  \n",
       "2         185  \n",
       "3         337  \n",
       "4         546  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_count(words):\n",
    "    return len(words.split(' ')) if type(words) == str else 0\n",
    "df['word_count'] = [word_count(x) for x in df[1]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    62204.000000\n",
       "mean       334.148479\n",
       "std        330.217525\n",
       "min          0.000000\n",
       "25%        148.000000\n",
       "50%        252.000000\n",
       "75%        402.000000\n",
       "max       9076.000000\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.word_count.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A slightly more predictable word count, a few hundred in most documents with some clear outliers, \n",
    "\n",
    "## What about the words themselves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words 20785372\n",
      "Unique words 1037934\n"
     ]
    }
   ],
   "source": [
    "def split_or_empty(words):\n",
    "    return words.split(' ') if type(words) == str else []\n",
    "\n",
    "word_lst = [split_or_empty(x) for x in df[1]]\n",
    "\n",
    "all_wrds = [word for lst in word_lst for word in lst]\n",
    "set_wrds = set(all_wrds)\n",
    "print(\"Total words %d\"%df.word_count.sum())\n",
    "print(\"Unique words %d\"%len(set_wrds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's a lot of unique words, and for 62K lines, not a lot of documents, but lets assume that there are a lot of stop words and since these are personalized documents a lot of names, addresses, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num common words 1037934\n"
     ]
    }
   ],
   "source": [
    "wrd_cntr = collections.Counter(all_wrds)\n",
    "common_words = wrd_cntr.most_common()\n",
    "\n",
    "print(\"Num common words %d\"%len(common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Distribution of Words: (Mean, Variance) 20.025716 846230.657444\n"
     ]
    }
   ],
   "source": [
    "freq_lst = [x[1] for x in common_words]\n",
    "freq_arr = np.array(freq_lst)\n",
    "print(\"Frequency Distribution of Words: (Mean, Variance) %f %f\"%(freq_arr.mean(), freq_arr.var()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    62204.000000\n",
       "mean       101.385120\n",
       "std         86.443821\n",
       "min          0.000000\n",
       "25%         48.000000\n",
       "50%         77.000000\n",
       "75%        128.000000\n",
       "max       1610.000000\n",
       "Name: words_in_vocab, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = [x[0] for x in common_words if x[1] > 5 and x[1] < 10000]\n",
    "vocab_set = set(vocab)\n",
    "def keep_vocab(words):\n",
    "    return set(split_or_empty(words)) & vocab_set\n",
    "\n",
    "df['words_in_vocab'] = [len(keep_vocab(x)) for x in df[1]]\n",
    "df.words_in_vocab.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>word_count</th>\n",
       "      <th>words_in_vocab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DELETION OF INTEREST</td>\n",
       "      <td>e04a09c87692 d6b72e591b91 5d066f0246f1 ed41171...</td>\n",
       "      <td>465</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RETURNED CHECK</td>\n",
       "      <td>a3b334c6eefd be95012ebf2b 41d67080e078 ff1c26e...</td>\n",
       "      <td>403</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BILL</td>\n",
       "      <td>586242498a88 9ccf259ca087 54709b24b45f 6bf9c0c...</td>\n",
       "      <td>185</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BILL</td>\n",
       "      <td>cd50e861f48b 6ca2dd348663 d38820625542 f077614...</td>\n",
       "      <td>337</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BILL</td>\n",
       "      <td>9db5536263d8 1c303d15eb65 3f89b4673455 b73e657...</td>\n",
       "      <td>546</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0                                                  1  \\\n",
       "0  DELETION OF INTEREST  e04a09c87692 d6b72e591b91 5d066f0246f1 ed41171...   \n",
       "1        RETURNED CHECK  a3b334c6eefd be95012ebf2b 41d67080e078 ff1c26e...   \n",
       "2                  BILL  586242498a88 9ccf259ca087 54709b24b45f 6bf9c0c...   \n",
       "3                  BILL  cd50e861f48b 6ca2dd348663 d38820625542 f077614...   \n",
       "4                  BILL  9db5536263d8 1c303d15eb65 3f89b4673455 b73e657...   \n",
       "\n",
       "   word_count  words_in_vocab  \n",
       "0         465             139  \n",
       "1         403             117  \n",
       "2         185              36  \n",
       "3         337             130  \n",
       "4         546             131  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So from this quick exercise, it seems like we can get a more manageable vocabulary by focusing on the words which are not too common and not too uncommon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>word_count</th>\n",
       "      <th>words_in_vocab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DELETION OF INTEREST</td>\n",
       "      <td>e04a09c87692 d6b72e591b91 5d066f0246f1 ed41171...</td>\n",
       "      <td>465</td>\n",
       "      <td>743b314e5665 b2c878a75d7e 44d3870bca21 4e9eb06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RETURNED CHECK</td>\n",
       "      <td>a3b334c6eefd be95012ebf2b 41d67080e078 ff1c26e...</td>\n",
       "      <td>403</td>\n",
       "      <td>29503e65a644 b2c878a75d7e 1850801b9c05 ea05dcb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BILL</td>\n",
       "      <td>586242498a88 9ccf259ca087 54709b24b45f 6bf9c0c...</td>\n",
       "      <td>185</td>\n",
       "      <td>0ad17934ee05 f1424da4e7d6 e0a34e168ea4 c85a9f2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BILL</td>\n",
       "      <td>cd50e861f48b 6ca2dd348663 d38820625542 f077614...</td>\n",
       "      <td>337</td>\n",
       "      <td>011113964d37 dafbb201715e 69c87281a156 83da9eb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BILL</td>\n",
       "      <td>9db5536263d8 1c303d15eb65 3f89b4673455 b73e657...</td>\n",
       "      <td>546</td>\n",
       "      <td>0025e6b23cc5 d671855584fd ba943a1c3175 dfa88dd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0                                                  1  \\\n",
       "0  DELETION OF INTEREST  e04a09c87692 d6b72e591b91 5d066f0246f1 ed41171...   \n",
       "1        RETURNED CHECK  a3b334c6eefd be95012ebf2b 41d67080e078 ff1c26e...   \n",
       "2                  BILL  586242498a88 9ccf259ca087 54709b24b45f 6bf9c0c...   \n",
       "3                  BILL  cd50e861f48b 6ca2dd348663 d38820625542 f077614...   \n",
       "4                  BILL  9db5536263d8 1c303d15eb65 3f89b4673455 b73e657...   \n",
       "\n",
       "   word_count                                     words_in_vocab  \n",
       "0         465  743b314e5665 b2c878a75d7e 44d3870bca21 4e9eb06...  \n",
       "1         403  29503e65a644 b2c878a75d7e 1850801b9c05 ea05dcb...  \n",
       "2         185  0ad17934ee05 f1424da4e7d6 e0a34e168ea4 c85a9f2...  \n",
       "3         337  011113964d37 dafbb201715e 69c87281a156 83da9eb...  \n",
       "4         546  0025e6b23cc5 d671855584fd ba943a1c3175 dfa88dd...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nona = df.dropna()\n",
    "df['words_in_vocab'] = [' '.join(keep_vocab(x)) for x in df[1]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So using this quick attempt to remove stop words and very infrequent words, let's move on to the prediction. \n",
    "\n",
    "## My first attempt attempt was to use a Naive Bayes approach using TF-IDF, thinking that these words (the ones remaining), might be useful for categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(df.words_in_vocab, df[0],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5020)\n",
    "\n",
    "# Encode the Prediction Label, so that we're using integers and not strings\n",
    "encoder = LabelEncoder()\n",
    "enc_train_y = encoder.fit_transform(train_y)\n",
    "enc_test_y = encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>word_count</th>\n",
       "      <th>words_in_vocab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DELETION OF INTEREST</td>\n",
       "      <td>e04a09c87692 d6b72e591b91 5d066f0246f1 ed41171...</td>\n",
       "      <td>465</td>\n",
       "      <td>fbe7c05e32d5 1807f8910862 3102eeb23202 1fa87d6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RETURNED CHECK</td>\n",
       "      <td>a3b334c6eefd be95012ebf2b 41d67080e078 ff1c26e...</td>\n",
       "      <td>403</td>\n",
       "      <td>1357209fd44f 687214cd0acb 7d4501e8b694 31cbd98...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BILL</td>\n",
       "      <td>586242498a88 9ccf259ca087 54709b24b45f 6bf9c0c...</td>\n",
       "      <td>185</td>\n",
       "      <td>b834a58b85b9 2e182c67811b 6753b57205cb 3d877a3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BILL</td>\n",
       "      <td>cd50e861f48b 6ca2dd348663 d38820625542 f077614...</td>\n",
       "      <td>337</td>\n",
       "      <td>034e2d7f187e c8207fafe699 7860028b1d17 a4ffd27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BILL</td>\n",
       "      <td>9db5536263d8 1c303d15eb65 3f89b4673455 b73e657...</td>\n",
       "      <td>546</td>\n",
       "      <td>1807f8910862 6c941621f20f a100eb50abec ad5f00d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0                                                  1  \\\n",
       "0  DELETION OF INTEREST  e04a09c87692 d6b72e591b91 5d066f0246f1 ed41171...   \n",
       "1        RETURNED CHECK  a3b334c6eefd be95012ebf2b 41d67080e078 ff1c26e...   \n",
       "2                  BILL  586242498a88 9ccf259ca087 54709b24b45f 6bf9c0c...   \n",
       "3                  BILL  cd50e861f48b 6ca2dd348663 d38820625542 f077614...   \n",
       "4                  BILL  9db5536263d8 1c303d15eb65 3f89b4673455 b73e657...   \n",
       "\n",
       "   word_count                                     words_in_vocab  \n",
       "0         465  fbe7c05e32d5 1807f8910862 3102eeb23202 1fa87d6...  \n",
       "1         403  1357209fd44f 687214cd0acb 7d4501e8b694 31cbd98...  \n",
       "2         185  b834a58b85b9 2e182c67811b 6753b57205cb 3d877a3...  \n",
       "3         337  034e2d7f187e c8207fafe699 7860028b1d17 a4ffd27...  \n",
       "4         546  1807f8910862 6c941621f20f a100eb50abec ad5f00d...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(ngram_range=(1,3))\n",
    "tfidf_vect.fit(df.words_in_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_tfidf = tfidf_vect.transform(train_x)\n",
    "test_x_tfidf = tfidf_vect.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  65.91110039385902\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "nb = naive_bayes.MultinomialNB()\n",
    "nb.fit(train_x_tfidf, train_y)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_nb = nb.predict(test_x_tfidf)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_nb, test_y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=10000,\n",
    "                             min_df=2, use_idf=True, ngram_range=(1,3))\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(df_nona[1], df_nona[0],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "enc_train_y = encoder.fit_transform(train_y)\n",
    "enc_test_y = encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.5, max_features=10000,\n",
       "                min_df=2, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(df_nona[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_tfidf = vectorizer.transform(train_x)\n",
    "test_x_tfidf = vectorizer.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  76.93050193050193\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "nb = naive_bayes.MultinomialNB()\n",
    "nb.fit(train_x_tfidf, train_y)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_nb = nb.predict(test_x_tfidf)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_nb, test_y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So even using NLTK's stopword system, TF-IDF vectors, and N-Grams of up to 3 words, we haven't cracked 80% accuracy\n",
    "\n",
    "## Next I move on to a combination of linear techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the SVD step: %2f %\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(100)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "# Run SVD on the training data, then project the training data.\n",
    "train_x_lsa = lsa.fit_transform(train_x_tfidf)\n",
    "\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: %2f %\".format(int(explained_variance * 100)))\n",
    "\n",
    "test_x_lsa = lsa.transform(test_x_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12432, 100)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x_lsa.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So here I'm making a big matrix of the training data's TF-IDF values, and then using Singular Value Decomposition to factor that matrix. This makes it easy to reduce the number of dimensions (since with up to 3 words as a feature, there are a LOT of dimensions). \n",
    "\n",
    "## Now that I've done SVD, I can project my TF-IDF vectors into this reduced vector space and use a simple categorizing algorithm (K-Nearest Neighbor) to train categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cats = len(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a k-NN classifier. Use k = 5 (majority wins), the cosine distance, \n",
    "# and brute-force calculation of distances.\n",
    "knn_lsa = KNeighborsClassifier(n_neighbors=num_cats, algorithm='brute', metric='cosine')\n",
    "knn_lsa.fit(train_x_lsa, enc_train_y)\n",
    "\n",
    "# Classify the test vectors.\n",
    "p = knn_lsa.predict(test_x_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (10273 / 12432) correct - 82.63%\n"
     ]
    }
   ],
   "source": [
    "# Measure accuracy\n",
    "numRight = 0;\n",
    "for i in range(0,len(p)):\n",
    "    if p[i] == enc_test_y[i]:\n",
    "        numRight += 1\n",
    "\n",
    "print(\"  (%d / %d) correct - %.2f%%\" % (numRight, len(enc_test_y), float(numRight) / float(len(enc_test_y)) * 100.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we've broken into the 80's with a respectable 82.6% accuracy, we can move on to pushing these models up to a production environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My next steps are to train a LSTM neural network on these word vectors. I tried to use Word2Vec to create new word vectors but I believe the relatively small size of the data makes this difficult, and that the linear nature of SVD allows these vectors to perform better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## However, there is clearly a use of the words that involves order that is important, and it would behoove any data scientist to at least try an LSTM and see if the accuracy could be improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "full_enc_y = encoder.fit_transform(full_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DELETION OF INTEREST', 'RETURNED CHECK', 'BILL', 'DECLARATION',\n",
       "       'DECLARATION'], dtype=object)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.inverse_transform([7, 13, 1, 6, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.5, max_features=10000,\n",
       "                min_df=2, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_x_tfidf = vectorizer.transform(full_train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(100)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "# Run SVD on the training data, then project the training data.\n",
    "full_train_x_lsa = lsa.fit_transform(full_train_x_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='brute', leaf_size=30, metric='cosine',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=14, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_lsa = KNeighborsClassifier(n_neighbors=num_cats, algorithm='brute', metric='cosine')\n",
    "knn_lsa.fit(full_train_x_lsa, full_enc_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

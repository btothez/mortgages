{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = pickle.load(open('../server/webservice/pickles/encoder.pkl', 'rb'))\n",
    "smaller_vectorizer = pickle.load(open('./smaller_vectorizer.pkl', 'rb'))\n",
    "new_lsa = pickle.load(open('./new_lsa.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "batch_size = 500\n",
    "chunk_size = 20\n",
    "output_size = 14\n",
    "lstm_layers = 2\n",
    "lstm_size = 256\n",
    "learning_rate = 0.001\n",
    "test_acc = []\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(\"float\", [None, chunk_size, 100])\n",
    "    labels_ = tf.placeholder(tf.int32, [None, output_size])\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"RNN_layers\"):\n",
    "        def lstm_cell():\n",
    "            # Your basic LSTM cell\n",
    "            lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size, reuse=tf.get_variable_scope().reuse)\n",
    "            # Add dropout to the cell\n",
    "            return tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "\n",
    "        # Stack up multiple LSTM layers, for deep learning\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(lstm_layers)])\n",
    "\n",
    "        # Getting an initial state of all zeros\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs_,\n",
    "                                             initial_state=initial_state)\n",
    "\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], output_size, activation_fn=tf.sigmoid)\n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)    \n",
    "    \n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_prediction(pd_vec):\n",
    "    \"\"\" Find the maximum prediction in the output vector \"\"\"\n",
    "    max_pred = max(pd_vec)\n",
    "    return np.array([int(n == max_pred) for n in pd_vec])\n",
    "\n",
    "def word_2_vect(word):\n",
    "    \"\"\" For a word, transform into a wordvec \"\"\"\n",
    "    return new_lsa.transform(smaller_vectorizer.transform([word]))[0]\n",
    "\n",
    "def vec_to_label(pdctns):\n",
    "    pred_vec = max_prediction(pdctns)\n",
    "    rolled = np.roll(pred_vec, 1)\n",
    "    return encoder.inverse_transform(\n",
    "        list(map(lambda tup: tup[0], \n",
    "                 filter(lambda tup: tup[1], enumerate(rolled)))))\n",
    "\n",
    "def chunks(l, n=chunk_size):\n",
    "    for i in range(0, len(l), n):\n",
    "        new_chunk = l[i:i+n]\n",
    "        if len(new_chunk) < n:\n",
    "            new_chunk = ([''] * (n - len(new_chunk))) + new_chunk\n",
    "        yield new_chunk    \n",
    "        \n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def result_from_batch(prediction_batch, padding_size=0):\n",
    "    \"\"\" Find Confidence and prediction label from LSTM output \"\"\"\n",
    "    result_vecs = prediction_batch[:(batch_size - padding_size)]\n",
    "    sm = softmax(result_vecs.sum(axis=0))\n",
    "    rolled = np.roll(sm, 1)\n",
    "    max_conf = rolled.max()\n",
    "    label_int = next(tup[0] for tup in enumerate(rolled) if tup[1] == max_conf)\n",
    "    return encoder.inverse_transform([label_int])[0], max_conf        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classification_engine(words):\n",
    "    \"\"\" Convert words into LSTM input tensor and return prediction and confidence \"\"\"\n",
    "    testing_set = []\n",
    "    word_arr = words.split(' ')\n",
    "\n",
    "    for chunk in chunks(word_arr, chunk_size):\n",
    "        testing_set.append([word_2_vect(wrd) for wrd in chunk])\n",
    "    \n",
    "    # Fill batch\n",
    "    padding_size = batch_size - len(testing_set)\n",
    "    testing_set.extend([testing_set[0] for n in range(padding_size)])\n",
    "    \n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        saver.restore(sess, \"checkpoints/final_sentiment.ckpt\")\n",
    "        test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "        feed = {inputs_: testing_set,\n",
    "                labels_: np.array([[0] * 14]),\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        batch_acc, test_state, prediction_batch = sess.run([accuracy, final_state, predictions], feed_dict=feed)\n",
    "    label, confidence = result_from_batch(prediction_batch, padding_size)\n",
    "    return {\n",
    "        \"prediction\": label,\n",
    "        \"confidence\": confidence\n",
    "    }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': 'BILL', 'confidence': 0.08548721}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_engine(\"\"\"\n",
    "28bce73d3237 \n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method transform in module sklearn.feature_extraction.text:\n",
      "\n",
      "transform(raw_documents, copy=True) method of sklearn.feature_extraction.text.TfidfVectorizer instance\n",
      "    Transform documents to document-term matrix.\n",
      "    \n",
      "    Uses the vocabulary and document frequencies (df) learned by fit (or\n",
      "    fit_transform).\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    raw_documents : iterable\n",
      "        an iterable which yields either str, unicode or file objects\n",
      "    \n",
      "    copy : boolean, default True\n",
      "        Whether to copy X and operate on the copy or perform in-place\n",
      "        operations.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    X : sparse matrix, [n_samples, n_features]\n",
      "        Tf-idf-weighted document-term matrix.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(smaller_vectorizer.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_vectorizer.transform()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
